{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713e2b95",
   "metadata": {},
   "source": [
    "# Identifying Precise Forecasters on r/Wallstreetbets\n",
    "**BrainStation Data Science Bootcamp - Capstone Project**\n",
    "\n",
    "**Author: L Gavrilova**\n",
    "\n",
    "**Date: 6 November 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8905b9",
   "metadata": {},
   "source": [
    "# Notebook 2 - Labelled dataset - Text Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e991fd",
   "metadata": {},
   "source": [
    "## 2.0. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "Removing website links <br>\n",
    "Filtering out emojis by creating a new column <br>\n",
    "2. Spellcheck.\n",
    "\n",
    ". [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f51324",
   "metadata": {},
   "source": [
    "### 2.1.1. Data Loading and Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3e553e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Regular Expressions Library\n",
    "import re\n",
    "\n",
    "# Emoji Handling Library\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/annotation file 3600 done 1142022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5020 entries, 0 to 5019\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   link_id    5001 non-null   object\n",
      " 1   parent_id  5001 non-null   object\n",
      " 2   User       5001 non-null   object\n",
      " 3   Text       5001 non-null   object\n",
      " 4   Intent     5001 non-null   object\n",
      " 5   Support    5001 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 235.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1075</th>\n",
       "      <td>t3_l692dj</td>\n",
       "      <td>t3_l692dj</td>\n",
       "      <td>Logical_Psychology55</td>\n",
       "      <td>Gotta tell ya: this is just the beginning. Dont look at the chart and perceive gain or losses as the chart moves. Take care of yourself and put the screen aside. The whole investment world is learning about why not to short a stock with over 100% short interest. A hard lesson that holding GME is teaching everyone. The outcome of this will be legendary.</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1344</th>\n",
       "      <td>t3_kxsd2p</td>\n",
       "      <td>t3_kxsd2p</td>\n",
       "      <td>SideOk3956</td>\n",
       "      <td>What is dead may never die; rise, GME!</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4271</th>\n",
       "      <td>t3_lsc5ji</td>\n",
       "      <td>t3_lsc5ji</td>\n",
       "      <td>Hurdrs123</td>\n",
       "      <td>GME only thing making me money today, too bad it’s still on 200 shares I’m 💼 holding from 280 🤡</td>\n",
       "      <td>y</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>t3_lacs3f</td>\n",
       "      <td>t3_lacs3f</td>\n",
       "      <td>Kureist</td>\n",
       "      <td>All in on $GME you stupid cunt.</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>t3_l6hopd</td>\n",
       "      <td>t1_gl1j9qe</td>\n",
       "      <td>randomperson0284</td>\n",
       "      <td>Yeah there is a ton of reading involved, but statistically 95% of options expire worthless and people lose money. \\n\\nThis sub has like over a million new people many of whom think this gamestop movement is a normal market occurrence lol its bizarre to read. I mean i hope you can double or triple what you have, idk. Gl</td>\n",
       "      <td>u</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id   parent_id                  User  \\\n",
       "1075  t3_l692dj   t3_l692dj  Logical_Psychology55   \n",
       "1344  t3_kxsd2p   t3_kxsd2p            SideOk3956   \n",
       "4271  t3_lsc5ji   t3_lsc5ji             Hurdrs123   \n",
       "573   t3_lacs3f   t3_lacs3f               Kureist   \n",
       "4783  t3_l6hopd  t1_gl1j9qe      randomperson0284   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                    Text  \\\n",
       "1075  Gotta tell ya: this is just the beginning. Dont look at the chart and perceive gain or losses as the chart moves. Take care of yourself and put the screen aside. The whole investment world is learning about why not to short a stock with over 100% short interest. A hard lesson that holding GME is teaching everyone. The outcome of this will be legendary.   \n",
       "1344                                                                                                                                                                                                                                                                                                                              What is dead may never die; rise, GME!   \n",
       "4271                                                                                                                                                                                                                                                                     GME only thing making me money today, too bad it’s still on 200 shares I’m 💼 holding from 280 🤡   \n",
       "573                                                                                                                                                                                                                                                                                                                                      All in on $GME you stupid cunt.   \n",
       "4783                                    Yeah there is a ton of reading involved, but statistically 95% of options expire worthless and people lose money. \\n\\nThis sub has like over a million new people many of whom think this gamestop movement is a normal market occurrence lol its bizarre to read. I mean i hope you can double or triple what you have, idk. Gl   \n",
       "\n",
       "     Intent Support  \n",
       "1075      u       y  \n",
       "1344      u       y  \n",
       "4271      y       u  \n",
       "573       y       y  \n",
       "4783      u       u  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1948</td>\n",
       "      <td>3153</td>\n",
       "      <td>4662</td>\n",
       "      <td>4952</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>GME</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>66</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>3246</td>\n",
       "      <td>2473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          link_id  parent_id           User  Text Intent Support\n",
       "count        5001       5001           5001  5001   5001    5001\n",
       "unique       1948       3153           4662  4952      6       4\n",
       "top     t3_ladzdt  t3_ladzdt  AutoModerator   GME      u       y\n",
       "freq           66         46             14    22   3246    2473"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_id'].nunique() == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_id      0.003785\n",
       "parent_id    0.003785\n",
       "User         0.003785\n",
       "Text         0.003785\n",
       "Intent       0.003785\n",
       "Support      0.003785\n",
       "dtype: float64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     link_id parent_id User Text Intent Support\n",
       "5001     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5002     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5003     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5004     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5005     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5006     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5007     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5008     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5009     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5010     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5011     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5012     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5013     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5014     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5015     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5016     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5017     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5018     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5019     NaN       NaN  NaN  NaN    NaN     NaN"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows that have NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [link_id, parent_id, User, Text, Intent, Support]\n",
       "Index: []"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intent\n",
       "u     3246\n",
       "y      983\n",
       "m      370\n",
       "i      318\n",
       "n       83\n",
       " u       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Intent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent\n",
      "u    3247\n",
      "y     983\n",
      "m     370\n",
      "i     318\n",
      "n      83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replacing ' u' with 'u' in the 'Intent' column\n",
    "df['Intent'] = df['Intent'].str.replace(' u', 'u', regex=False)\n",
    "# checking again:\n",
    "value_counts = df['Intent'].value_counts() \n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def purge_content(text):\n",
    "    text_without_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text_without_hashtags = re.sub(r'#\\S+', '', text_without_urls)\n",
    "    text_without_mentions = re.sub(r'@\\S+', '', text_without_hashtags)\n",
    "    clean_text = re.sub(r'\\n+', ' ', text_without_mentions)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "for i in range(len(df_clean['Text'])):\n",
    "    df_clean['Text'][i] = purge_content(df_clean['Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>t3_l66caa</td>\n",
       "      <td>t1_gkyxvml</td>\n",
       "      <td>EllipticalOrbitMan</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>t3_l0mc06</td>\n",
       "      <td>t1_gju8jei</td>\n",
       "      <td>wolfiasty</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>t3_l6kqyk</td>\n",
       "      <td>t1_gl17oj6</td>\n",
       "      <td>EconomicallyLiterate</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>t3_khq3x2</td>\n",
       "      <td>t1_ggo6fi4</td>\n",
       "      <td>JonBoy82</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>t3_lat43j</td>\n",
       "      <td>t1_glq69gz</td>\n",
       "      <td>Free_Joty</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id   parent_id                  User Text Intent Support\n",
       "402   t3_l66caa  t1_gkyxvml    EllipticalOrbitMan           i       i\n",
       "1264  t3_l0mc06  t1_gju8jei             wolfiasty           i       i\n",
       "2187  t3_l6kqyk  t1_gl17oj6  EconomicallyLiterate           i       i\n",
       "4157  t3_khq3x2  t1_ggo6fi4              JonBoy82           i       i\n",
       "4265  t3_lat43j  t1_glq69gz             Free_Joty           i       i"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "df_clean[df_clean['Text'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the 'Text' column is an empty string\n",
    "df_clean = df_clean[df_clean['Text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording the cleaned dataset as a new csv file to be used in other notebooks:\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../data/labelled_dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b4226",
   "metadata": {},
   "source": [
    "### 2.1.2. Filtering out `emojis` by creating a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e983b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map emojis to their descriptions\n",
    "def emoji_description(emoji):\n",
    "    emoji_map = {\n",
    "        \"🚀\": \" super optimistic, \",\n",
    "        \"🦍\": \" brotherhood, \",\n",
    "        \"🤞\": \" hope, \",\n",
    "        \"🌙\": \" very optimistic, \",\n",
    "        \"🌕\": \" very optimistic, \",\n",
    "        \"💎🤚🏼\": \" patient investors, \",\n",
    "        \"💎🖐\": \" patient investors, \",\n",
    "        \"💎🙌\": \" patient investors, \",\n",
    "        \"🙌\": \" patient investors, \",\n",
    "        \"💎\": \" patient investors, \",\n",
    "        \"🧻🤚🏼\": \" impatient investors, \",\n",
    "        \"🧻🖐\": \" impatient investors, \",\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    # If the full emoji is in the map, return the description\n",
    "    if emoji in emoji_map:\n",
    "        return emoji_map[emoji]\n",
    "    # If not, split any combined emojis and look up their individual descriptions\n",
    "    else:\n",
    "        return ''.join([emoji_map.get(char, '') for char in emoji])  # Default to empty string if not in mapping\n",
    "\n",
    "def extract_and_replace_emojis(df, text_column_name='Text', emoji_column_name='emoji_text'):\n",
    "    # Initialize an empty column for extracted emojis if a column name is provided\n",
    "    if emoji_column_name:\n",
    "        df[emoji_column_name] = ''\n",
    "\n",
    "    # Function to extract and replace emojis in a text\n",
    "    def process_text(text):\n",
    "        emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001F004]+')\n",
    "\n",
    "        # Find all emojis in the text using the regex pattern\n",
    "        emoji_matches = emoji_pattern.findall(text)\n",
    "        emojis_extracted = ''\n",
    "        text_with_replaced_emojis = text\n",
    "\n",
    "        # Iterate over the found emojis\n",
    "        for emoji_str in emoji_matches:\n",
    "            # For each emoji in the emoji string\n",
    "            for emoji_char in emoji_str:\n",
    "                emoji_desc = emoji_description(emoji_char)  # Get description for individual emoji\n",
    "                text_with_replaced_emojis = text_with_replaced_emojis.replace(emoji_char, emoji_desc, 1)\n",
    "                emojis_extracted += emoji_char + ' '  # Add space to separate emojis\n",
    "\n",
    "        # Return the modified text and the extracted emojis\n",
    "        return text_with_replaced_emojis, emojis_extracted.strip()\n",
    "\n",
    "    # Apply the processing function to the specified column and create new columns for text and emojis\n",
    "    result = df[text_column_name].apply(process_text)\n",
    "    df[text_column_name] = result.apply(lambda x: x[0])\n",
    "    \n",
    "    if emoji_column_name:\n",
    "        df[emoji_column_name] = result.apply(lambda x: x[1])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c98bc5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_and_replace_emojis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_labelled_text_cleaning_final.ipynb Cell 28\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_labelled_text_cleaning_final.ipynb#X36sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Applying the function to extract and replace emojis from 'Text' column\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_labelled_text_cleaning_final.ipynb#X36sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_clean \u001b[39m=\u001b[39m extract_and_replace_emojis(df_clean, text_column_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m, emoji_column_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39memoji_text\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_and_replace_emojis' is not defined"
     ]
    }
   ],
   "source": [
    "# Applying the function to extract and replace emojis from 'Text' column\n",
    "df_clean = extract_and_replace_emojis(df_clean, text_column_name='Text', emoji_column_name='emoji_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4be45fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji_text\n",
       "                                                                     4254\n",
       "🚀 🚀 🚀                                                                  66\n",
       "🚀                                                                      50\n",
       "🚀 🚀                                                                    28\n",
       "🚀 🚀 🚀 🚀                                                                26\n",
       "                                                                     ... \n",
       "🧻 🤚 🏼 🧻 🤚 🏼 🧻 🤚 🏼 🧻 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀       1\n",
       "💎 🚀 🚀 🌙                                                                 1\n",
       "🚀 💪 🏋 💎                                                                 1\n",
       "📝 👋 💎 👏 🚀 🚀 🚀 🌈 🐻 📉 🚀 🚀 🚀 🌕 🔥 🔥                                         1\n",
       "🚀 🚀 🚀 🖐 💎 🖐 💵 🖐 🍿 🍗 🚀 🦍 🌚 🚀 🚀                                           1\n",
       "Name: count, Length: 383, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the new column with emojis extracted from the text\n",
    "df_clean.sample().T\n",
    "df_clean['emoji_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4781</th>\n",
       "      <td>t3_m3f9fp</td>\n",
       "      <td>t3_m3f9fp</td>\n",
       "      <td>jessekg</td>\n",
       "      <td>Opposite of a ladder attack — the Gamma swarm lol  [</td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3853</th>\n",
       "      <td>t3_lnqgz8</td>\n",
       "      <td>t1_go1wkpr</td>\n",
       "      <td>skwolf522</td>\n",
       "      <td>Everytime i think this whole gme saga has peaked....  shit like this happens.</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>Wolf_Of_1337_Street</td>\n",
       "      <td>so many people here are sitting on life-changing profits from GME but they are refusing to take profit because somehow this got spun into class warfare vs. \"the hedge funds.\" It's really sad to see people missing out on this money for no reason. GME cannot stay at this level forever.</td>\n",
       "      <td>u</td>\n",
       "      <td>n</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id   parent_id                 User  \\\n",
       "4781  t3_m3f9fp   t3_m3f9fp              jessekg   \n",
       "3853  t3_lnqgz8  t1_go1wkpr            skwolf522   \n",
       "306   t3_ladzdt   t3_ladzdt  Wolf_Of_1337_Street   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                              Text  \\\n",
       "4781                                                                                                                                                                                                                                          Opposite of a ladder attack — the Gamma swarm lol  [   \n",
       "3853                                                                                                                                                                                                                 Everytime i think this whole gme saga has peaked....  shit like this happens.   \n",
       "306   so many people here are sitting on life-changing profits from GME but they are refusing to take profit because somehow this got spun into class warfare vs. \"the hedge funds.\" It's really sad to see people missing out on this money for no reason. GME cannot stay at this level forever.   \n",
       "\n",
       "     Intent Support emoji_text  \n",
       "4781      i       i             \n",
       "3853      u       y             \n",
       "306       u       n             "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Replacing slang with custom made \"WSB Dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WSB lingo dictionary\n",
    "wsb_dict_df = pd.read_csv('../data/WSB_dictionary.csv')\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "wsb_dict = dict(zip(wsb_dict_df['WSB lingo'], wsb_dict_df['English']))\n",
    "\n",
    "# Function to replace WSB lingo with English\n",
    "def replace_wsb_lingo(text):\n",
    "    # Use a regex pattern to match only whole words\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(key) for key in wsb_dict.keys()) + r')\\b'\n",
    "    # Replace occurrences of each lingo with the English equivalent\n",
    "    return re.sub(pattern, lambda x: wsb_dict[x.group()], text)\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df_clean['Text'] = df_clean['Text'].apply(replace_wsb_lingo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Examples of texts before and after the cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                                                                                                                                                                                                                                                                                                                             t3_l6cb1x\n",
      "parent_id                                                                                                                                                                                                                                                                                                                                           t3_l6cb1x\n",
      "User                                                                                                                                                                                                                                                                                                                                     nights_that_say_knee\n",
      "Text         👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐\\n\\nBUY 🚀🚀 GME 🚀🚀 to get your 🎟️🎟️🌕l🌝\\n\\nAIN'T NO LIE BABY 👶, 💰BUY, 🤑 BUY, 💵 BUY!\n",
      "Intent                                                                                                                                                                                                                                                                                                                                                      u\n",
      "Support                                                                                                                                                                                                                                                                                                                                                     y\n",
      "Name: 2026, dtype: object\n",
      "link_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       t3_l6cb1x\n",
      "parent_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     t3_l6cb1x\n",
      "User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               nights_that_say_knee\n",
      "Text           patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  BUY  super optimistic,  super optimistic,  GME  super optimistic,  super optimistic,  to get your ️️ very optimistic, l AIN'T NO LIE BABY , BUY,  BUY,  BUY!\n",
      "Intent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                u\n",
      "Support                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               y\n",
      "emoji_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 🚀 🚀 🚀 🚀 🎟 🎟 🌕 🌝 👶 💰 🤑 💵\n",
      "Name: 2026, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[2026]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[2026]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                            t3_l8ynt4\n",
      "parent_id                                          t3_l8ynt4\n",
      "User                                       wowexcellentstuff\n",
      "Text         did NOT read. $GME to mf Andromeda 🚀🚀🚀🌌🌌\\n\\n💎🤲💎\n",
      "Intent                                                     u\n",
      "Support                                                    y\n",
      "Name: 3986, dtype: object\n",
      "link_id                                                                                                                                   t3_l8ynt4\n",
      "parent_id                                                                                                                                 t3_l8ynt4\n",
      "User                                                                                                                              wowexcellentstuff\n",
      "Text          did NOT read. $GME to mf Andromeda  super optimistic,  super optimistic,  super optimistic,   patient investors,  patient investors, \n",
      "Intent                                                                                                                                            u\n",
      "Support                                                                                                                                           y\n",
      "emoji_text                                                                                                                          🚀 🚀 🚀 🌌 🌌 💎 🤲 💎\n",
      "Name: 3986, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[3986]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[3986]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                    t3_kkwy50\n",
      "parent_id                                  t3_kkwy50\n",
      "User                                SnooMacarons1548\n",
      "Text         GME🚀🚀🚀\\n\\nIt's a money printing company\n",
      "Intent                                             u\n",
      "Support                                            y\n",
      "Name: 3386, dtype: object\n",
      "link_id                                                                                        t3_kkwy50\n",
      "parent_id                                                                                      t3_kkwy50\n",
      "User                                                                                    SnooMacarons1548\n",
      "Text          GME super optimistic,  super optimistic,  super optimistic,  It's a money printing company\n",
      "Intent                                                                                                 u\n",
      "Support                                                                                                y\n",
      "emoji_text                                                                                         🚀 🚀 🚀\n",
      "Name: 3386, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[3386]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[3386]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d8ec9",
   "metadata": {},
   "source": [
    "## 2.2. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording the cleaned dataset as a new csv file to be used in future:\n",
    "# Save the DataFrame to a CSV file\n",
    "df_clean.to_csv('../data/labelled_dataset_wo_emoji.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation and anything except for letters is stripped away, also empty spaces go away. \n",
    "\n",
    "# Emojis are stripped off!!!! NB! \n",
    "\n",
    "if False:\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    # to replace any character that is not a lowercase or uppercase letter with a single space\n",
    "    # then to replace one or more whitespace characters (\\s+) with a single space\n",
    "    # then to replace '\\n' with empty spaces\n",
    "    # then to remove all types of whitespace characters at the ends of the string\n",
    "    # cleaned_df[\"Text\"] = cleaned_df[\"Text\"].replace(\"\\n\", \"\").str.replace(r\"[^a-zA-Z]\", \" \").str.replace(r\"\\s+\", \" \")\n",
    "\n",
    "\n",
    "    # First, replace newline characters with an empty string for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(\"\\n\", \"\", regex=False)\n",
    "\n",
    "    # Then, replace non-alphabetic characters with a space for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
    "\n",
    "    # Then, replace multiple spaces with a single space for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Finally, strip leading and trailing spaces from each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.strip()\n",
    "\n",
    "    df=cleaned_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f4bf8afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:7\u001b[0;36m\u001b[0m\n\u001b[0;31m    def purge_content(text):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# dfdfd\n",
    "\n",
    "if False:\n",
    "        df_clean = df.copy()\n",
    "\n",
    "    # Function to clean text\n",
    "    def purge_content(text):\n",
    "        # Define patterns for URLs, hashtags, mentions, and newlines\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        hashtag_pattern = r'#\\S+'\n",
    "        mention_pattern = r'@\\S+'\n",
    "        newline_pattern = r'\\n+'\n",
    "        \n",
    "        # Remove URLs\n",
    "        purged_text = re.sub(url_pattern, '', text)\n",
    "        # Remove hashtags\n",
    "        purged_text = re.sub(hashtag_pattern, '', purged_text)\n",
    "        # Remove mentions\n",
    "        purged_text = re.sub(mention_pattern, '', purged_text)\n",
    "        # Remove newlines\n",
    "        purged_text = re.sub(newline_pattern, ' ', purged_text)\n",
    "        \n",
    "        return purged_text\n",
    "\n",
    "    # Clean the 'Text' column\n",
    "    df_clean['Text'] = df_clean['Text'].apply(purge_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
