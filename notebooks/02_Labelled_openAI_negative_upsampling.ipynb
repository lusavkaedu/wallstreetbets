{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project \n",
    "\n",
    "**BrainStation Data Science Bootcamp - Machine Learning Project**\n",
    "\n",
    "**Author: L Gavrilova**\n",
    "\n",
    "**Date: 20 November 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2. Negative Labels Upsampling using OpenAI API  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My EDA of the labelled dataset described in the Notebook 1 highlighted a serious shortage of the negatively labelled comments/messages (only 1.6% of the data).  When I tried to run my modelling using such deeply imbalanced dataset I wasn’t getting good results on recall and precision for a minority class. Therefore, **I decided to use open Ai’s GPT models as a substitute for human labelling**.  I used GPT 3.5 as a first labeller, and then I used GPT-4 model as a second \"pair of eyes\", so to say, to label my dataset. The process is detailed in this notebook in the code below, and it can be summarised as follows: \n",
    "\n",
    "1. I used a large dataset of Reddit WallStreetBets posts from 2021 as my source of text/posts.  The dataset is described in detail in my project Notebook 4. In short, it is a large dataset of finance related subreddits posts collected in 2021.  The source of the dataset is here: https://www.kaggle.com/datasets/leukipp/reddit-finance-data.  See my Notebook 4 for a full description.  I will refer to this dataset as \"the Reddit dataset\" throughout my notebooks (to distinguish it from the small “labelled dataset” described in the Notebook 1). \n",
    "\n",
    "2. I loaded the Reddit dataset and filtered out those rows that has empty text fields, too long and too short messages, and concentrated on the period after March 2021, when the bullishness that prevailed in 1Q 2021 has somewhat subsided. I thought that probability of finding negative/bearish messages was higher once we exclude an extremely bullish period of Q1 2021. \n",
    "\n",
    "3. I selected a portion of the large Reddit dataset to send to OpenAI API accompanied with a prompt to rate it as expressing bullish or bearish views on securities. \n",
    "\n",
    "4. I collected GPT 3.5 responses and filtered out all but negatively rated messages.  \n",
    "\n",
    "5. I then fed the negative only messages to GPT 4 model, using the same prompt, and collected its ratings. \n",
    "\n",
    "6. I kept only those messages where both models \"agreed\" that the message was bearish/negative with respect to the securities mentioned in its text. \n",
    "\n",
    "The final result of this process was saved as a CSV file. \n",
    "\n",
    "The table below describes the process in detail:\n",
    "\n",
    "| Description | Rows after this step | % of starting rows kept | % of starting rows deleted at this step |\n",
    "|-------------|----------------------:|-----------------------:|---------------------------------------:|\n",
    "| Data load | 1,260,374 | 100.00% | |\n",
    "| rows where 'link_flair_text' is irrelevant | 1,246,161 | 98.87% | -1.1% |\n",
    "| rows where 'title' starts with 'Daily ' | 1,245,087 | 98.79% | -0.1% |\n",
    "| rows where 'selftext' is '[removed]' | 618,703 | 49.09% | -49.7% |\n",
    "| rows where 'date' is before April 1, 2021 | 256,656 | 20.36% | -28.7% |\n",
    "| rows where token_count is greater than 400 | 241,354 | 19.15% | -1.2% |\n",
    "| rows where token_count is smaller than 20 | 111,183 | 8.82% | -10.3% |\n",
    "| some rows processed with Chat GPT 3.5 API | 3,648 | 0.29% | -8.5% |\n",
    "| only \"Negative\" ratings fed back to GPT 4 | 725 | 0.06% | -0.2% |\n",
    "| rows where both GPT model ratings are 'negative' | 450 | 0.04% | 0.0% |\n",
    "\n",
    "I hoped to process more data using GPT models, however, GPT 3.5 model was very slow when I tried to do it, so I was limited to 450 negative messages after 2 full days of trying to label more data. I think this was a sufficient number of rows to start training my models.  This process can be continued, once the initial results are formulated, in order to improve overall model quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Generating more negative class labels using Chat GPT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import openai\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1260374, 18)\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "df_all = pd.read_csv('../data/reddit_cleaned_slim.csv')\n",
    "print(df_all.shape)\n",
    "# df = df.head(9)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Eliminating rows and columns that are not relevant to investing and stock trading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FolderName\n",
      "wallstreetbets          719899\n",
      "gme                     268376\n",
      "stocks                   68524\n",
      "pennystocks              50712\n",
      "stockmarket              41388\n",
      "investing                38162\n",
      "options                  27965\n",
      "robinhoodpennystocks     21235\n",
      "robinhood                17564\n",
      "finance                   6549\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Getting frequency counts for 'FolderName'\n",
    "folder_name_counts = df_all['FolderName'].value_counts()\n",
    "\n",
    "# Displaying the frequency counts\n",
    "print(folder_name_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1246161, 18)\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'link_flair_text' is irrelevant\n",
    "\n",
    "# List of irrelevant flair texts\n",
    "irrelevant_flairs = ['Credit', 'Taxes', 'Other', 'Housing', 'Retirement', 'Planning', 'Saving', 'Debt', 'Auto', 'Employment', 'Insurance', \"Budgeting\", \"Advice\", 'Advice Request']\n",
    "\n",
    "# Filter out rows where 'link_flair_text' is in the list of irrelevant flairs\n",
    "df_all = df_all[~df_all['link_flair_text'].isin(irrelevant_flairs)].copy()\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1245087, 18)\n"
     ]
    }
   ],
   "source": [
    "# Convert 'title' column to string\n",
    "df_all['title'] = df_all['title'].astype(str)\n",
    "\n",
    "# Filter out rows where 'title' starts with 'Daily '\n",
    "df_all = df_all[~df_all['title'].str.startswith('Daily ')].copy()\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1. Optional filters (untoggle as needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(618703, 18)\n"
     ]
    }
   ],
   "source": [
    "# Filter out rows where 'selftext' is '[removed]'\n",
    "#df_all = df_all[df_all['selftext'] != '[removed]'].copy()\n",
    "#df_all = df_all[df_all['selftext'] != '[deleted]'].copy()\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime\n",
    "df_all['date'] = pd.to_datetime(df_all['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256656, 18)\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where 'date' is before April 1, 2021\n",
    "df_all = df_all[df_all['date'] > '2021-04-01'].copy()\n",
    "print(df_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select rows containing \"GME\", \"Gamestop\", or \"$GME\" in the 'Text' column\n",
    "df = df[df['Text'].str.contains(r'GME|Gamestop|\\$GME', case=False, regex=True)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Only keeping the text fields in a smaller dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the 'id', 'title', and 'selftext' columns\n",
    "df = df_all[['id', 'title', 'selftext']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 28730 entries, 46 to 1048971\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           28730 non-null  object\n",
      " 1   title        28730 non-null  object\n",
      " 2   selftext     13489 non-null  object\n",
      " 3   Text         28730 non-null  object\n",
      " 4   token_count  28730 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>473088</th>\n",
       "      <td>rcykn5</td>\n",
       "      <td>Why invest in Liberty Oilfield Services?</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401438</th>\n",
       "      <td>qfmagt</td>\n",
       "      <td>Blackberry shares thoughts?</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>792572</th>\n",
       "      <td>nuhd4n</td>\n",
       "      <td>To the moon!!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197229</th>\n",
       "      <td>mx0jcm</td>\n",
       "      <td>It’s going to happen</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990785</th>\n",
       "      <td>mkbmic</td>\n",
       "      <td>Easter &amp; Egg Hunting with the Fam was fun.. Ok...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title   selftext\n",
       "473088  rcykn5           Why invest in Liberty Oilfield Services?  [removed]\n",
       "401438  qfmagt                        Blackberry shares thoughts?  [removed]\n",
       "792572  nuhd4n                                      To the moon!!        NaN\n",
       "197229  mx0jcm                               It’s going to happen        NaN\n",
       "990785  mkbmic  Easter & Egg Hunting with the Fam was fun.. Ok...        NaN"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               Text\n",
      "46   msblc3  GME YOLO update — Apr 16 2021 — final update, nan\n",
      "69   pu7l07  What Are Your Moves Tomorrow, September 24, 20...\n",
      "87   mqp6lv  COIN IPO Megathread 4/14/2021, This is a megat...\n",
      "155  pk8tne  For anyone who doesn’t understand why Hedgefon...\n",
      "158  nuxr2t  r/GME Megathread for Tuesday - June 08, 2021, ...\n"
     ]
    }
   ],
   "source": [
    "# Concatenating 'title' and 'selftext' with a comma separator\n",
    "df['Text'] = df.apply(lambda row: f\"{row['title']}, {row['selftext']}\", axis=1)\n",
    "\n",
    "# Displaying the first few rows of the new concatenated column\n",
    "print(df[['id', 'Text']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Eliminating lengthy messages (too expensive for chat GPT processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_Labelled_openAI_negative_upsampling.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_Labelled_openAI_negative_upsampling.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(tokenizer\u001b[39m.\u001b[39mencode(text))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_Labelled_openAI_negative_upsampling.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Applying the function to your dataframe\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_Labelled_openAI_negative_upsampling.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mtoken_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(estimate_tokens)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Function to estimate tokens\n",
    "def estimate_tokens(text):\n",
    "    return len(tokenizer.encode(text))\n",
    "\n",
    "# Applying the function to your dataframe\n",
    "df['token_count'] = df['Text'].apply(estimate_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>968630</th>\n",
       "      <td>qipv5n</td>\n",
       "      <td>Posts about posts fud</td>\n",
       "      <td>You've seen it many times if you've been here ...</td>\n",
       "      <td>Posts about posts fud, You've seen it many tim...</td>\n",
       "      <td>1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29775</th>\n",
       "      <td>nzsghf</td>\n",
       "      <td>AMD - constantly outperforming, constantly und...</td>\n",
       "      <td>\\n**Basics**\\n\\nAs long as you haven't been li...</td>\n",
       "      <td>AMD - constantly outperforming, constantly und...</td>\n",
       "      <td>869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19587</th>\n",
       "      <td>oeas8a</td>\n",
       "      <td>Intel (INTC) stock prospects: are you bullish ...</td>\n",
       "      <td>I bought INTC when I first started investing a...</td>\n",
       "      <td>Intel (INTC) stock prospects: are you bullish ...</td>\n",
       "      <td>260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9015</th>\n",
       "      <td>o6j2an</td>\n",
       "      <td>Ken Griffin has become the scapegoat for the f...</td>\n",
       "      <td>Wrinkle brains and reddit detectives, I need y...</td>\n",
       "      <td>Ken Griffin has become the scapegoat for the f...</td>\n",
       "      <td>712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12474</th>\n",
       "      <td>pres5t</td>\n",
       "      <td>Evergrande FUD, What the China property market...</td>\n",
       "      <td>From [r/baba](www.reddit.com/r/baba):\\n\\nThere...</td>\n",
       "      <td>Evergrande FUD, What the China property market...</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                              title  \\\n",
       "968630  qipv5n                              Posts about posts fud   \n",
       "29775   nzsghf  AMD - constantly outperforming, constantly und...   \n",
       "19587   oeas8a  Intel (INTC) stock prospects: are you bullish ...   \n",
       "9015    o6j2an  Ken Griffin has become the scapegoat for the f...   \n",
       "12474   pres5t  Evergrande FUD, What the China property market...   \n",
       "\n",
       "                                                 selftext  \\\n",
       "968630  You've seen it many times if you've been here ...   \n",
       "29775   \\n**Basics**\\n\\nAs long as you haven't been li...   \n",
       "19587   I bought INTC when I first started investing a...   \n",
       "9015    Wrinkle brains and reddit detectives, I need y...   \n",
       "12474   From [r/baba](www.reddit.com/r/baba):\\n\\nThere...   \n",
       "\n",
       "                                                     Text  token_count  \n",
       "968630  Posts about posts fud, You've seen it many tim...         1257  \n",
       "29775   AMD - constantly outperforming, constantly und...          869  \n",
       "19587   Intel (INTC) stock prospects: are you bullish ...          260  \n",
       "9015    Ken Griffin has become the scapegoat for the f...          712  \n",
       "12474   Evergrande FUD, What the China property market...          657  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_rows = df[df['token_count'] > 400]\n",
    "long_rows.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18918</th>\n",
       "      <td>o5xlff</td>\n",
       "      <td>HODL GME my new plates waiting for the Lambo now.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HODL GME my new plates waiting for the Lambo n...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18204</th>\n",
       "      <td>ner6w3</td>\n",
       "      <td>GME 101 w/ Marantz Rantz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GME 101 w/ Marantz Rantz, nan</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20760</th>\n",
       "      <td>qsedhb</td>\n",
       "      <td>Told parents to buy GME multiple times before…</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Told parents to buy GME multiple times before…...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20645</th>\n",
       "      <td>qiuoj9</td>\n",
       "      <td>Perhaps Dr. Burry works for GameStop’s Twitter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Perhaps Dr. Burry works for GameStop’s Twitter...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12550</th>\n",
       "      <td>r06rwd</td>\n",
       "      <td>GameStop will print, test 260 get ready!</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GameStop will print, test 260 get ready!, nan</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                              title selftext  \\\n",
       "18918  o5xlff  HODL GME my new plates waiting for the Lambo now.      NaN   \n",
       "18204  ner6w3                           GME 101 w/ Marantz Rantz      NaN   \n",
       "20760  qsedhb     Told parents to buy GME multiple times before…      NaN   \n",
       "20645  qiuoj9  Perhaps Dr. Burry works for GameStop’s Twitter...      NaN   \n",
       "12550  r06rwd           GameStop will print, test 260 get ready!      NaN   \n",
       "\n",
       "                                                    Text  token_count  \n",
       "18918  HODL GME my new plates waiting for the Lambo n...           16  \n",
       "18204                      GME 101 w/ Marantz Rantz, nan           11  \n",
       "20760  Told parents to buy GME multiple times before…...           13  \n",
       "20645  Perhaps Dr. Burry works for GameStop’s Twitter...           17  \n",
       "12550      GameStop will print, test 260 get ready!, nan           11  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short_rows = df[df['token_count'] < 20]\n",
    "short_rows.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21410, 5)\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows where token_count is greater than 400\n",
    "df = df[df['token_count'] <= 400].reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21271, 5)\n"
     ]
    }
   ],
   "source": [
    "# Dropping rows where token_count is smaller than 20\n",
    "df = df[df['token_count'] > 20].reset_index(drop=True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1047792\n"
     ]
    }
   ],
   "source": [
    "token_total = df['token_count'].agg(\"sum\")\n",
    "print(token_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "token_average = df['token_count'].agg(\"median\")\n",
    "print(token_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21271 entries, 0 to 21270\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   id           21271 non-null  object\n",
      " 1   title        21271 non-null  object\n",
      " 2   selftext     6173 non-null   object\n",
      " 3   Text         21271 non-null  object\n",
      " 4   token_count  21271 non-null  int64 \n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 831.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.drop(df.index[0:180], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15762, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Messages fed to GPT 3.5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below was written with the help of Chat GPT, as I wasn’t able to navigate all the complexities of the OpenAi API on my own.  In particular, I struggled with the throttling of API requests, and I could not figure out how to slow down the requests (number of API requests per minute), so Chat GPT was used to add the throttling function and the batch processing function.  \n",
    "\n",
    "The code was designed to analyse sentiment in Reddit messages selected above. \n",
    "\n",
    "1) The ask_gpt function sends text to the GPT-3.5 model and returns its response, handling exceptions if they occur. \n",
    "2) The throttled_request function includes a delay between requests. \n",
    "3) The main function, process_batch, processes data in small batches (only several rows at a time, in case the system becomes non-responsive). The function sends a prompt to GPT model and the response is a sentiment tag 'Positive', 'Negative', or 'Unclear'.  \n",
    "4) The results are then saved to a CSV file in batches, with the ability to append results. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 of 5254\n",
      "Processed batch 2 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 3 of 5254\n",
      "Processed batch 4 of 5254\n",
      "Processed batch 5 of 5254\n",
      "Processed batch 6 of 5254\n",
      "Processed batch 7 of 5254\n",
      "Processed batch 8 of 5254\n",
      "Processed batch 9 of 5254\n",
      "Processed batch 10 of 5254\n",
      "Processed batch 11 of 5254\n",
      "Processed batch 12 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 13 of 5254\n",
      "Processed batch 14 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 15 of 5254\n",
      "Processed batch 16 of 5254\n",
      "Processed batch 17 of 5254\n",
      "Processed batch 18 of 5254\n",
      "Processed batch 19 of 5254\n",
      "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 20 Nov 2023 11:08:18 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '82902d511e606551-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "Processed batch 20 of 5254\n",
      "Processed batch 21 of 5254\n",
      "Processed batch 22 of 5254\n",
      "Processed batch 23 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 24 of 5254\n",
      "Processed batch 25 of 5254\n",
      "Processed batch 26 of 5254\n",
      "Processed batch 27 of 5254\n",
      "Processed batch 28 of 5254\n",
      "Processed batch 29 of 5254\n",
      "Processed batch 30 of 5254\n",
      "Processed batch 31 of 5254\n",
      "Processed batch 32 of 5254\n",
      "Processed batch 33 of 5254\n",
      "An error occurred: The server is overloaded or not ready yet.\n",
      "Processed batch 34 of 5254\n",
      "Processed batch 35 of 5254\n",
      "Processed batch 36 of 5254\n",
      "Processed batch 37 of 5254\n",
      "Processed batch 38 of 5254\n",
      "Processed batch 39 of 5254\n",
      "Processed batch 40 of 5254\n",
      "Processed batch 41 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 42 of 5254\n",
      "Processed batch 43 of 5254\n",
      "Processed batch 44 of 5254\n",
      "Processed batch 45 of 5254\n",
      "Processed batch 46 of 5254\n",
      "Processed batch 47 of 5254\n",
      "Processed batch 48 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 49 of 5254\n",
      "Processed batch 50 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 51 of 5254\n",
      "Processed batch 52 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 53 of 5254\n",
      "Processed batch 54 of 5254\n",
      "Processed batch 55 of 5254\n",
      "Processed batch 56 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "Processed batch 57 of 5254\n",
      "An error occurred: Request timed out: HTTPSConnectionPool(host='api.openai.com', port=443): Read timed out. (read timeout=600)\n",
      "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 20 Nov 2023 14:05:14 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '8291307d998523d4-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "Processed batch 58 of 5254\n",
      "An error occurred: The server is overloaded or not ready yet.\n",
      "An error occurred: Bad gateway. {\"error\":{\"code\":502,\"message\":\"Bad gateway.\",\"param\":null,\"type\":\"cf_bad_gateway\"}} 502 {'error': {'code': 502, 'message': 'Bad gateway.', 'param': None, 'type': 'cf_bad_gateway'}} {'Date': 'Mon, 20 Nov 2023 14:22:25 GMT', 'Content-Type': 'application/json', 'Content-Length': '84', 'Connection': 'keep-alive', 'X-Frame-Options': 'SAMEORIGIN', 'Referrer-Policy': 'same-origin', 'Cache-Control': 'private, max-age=0, no-store, no-cache, must-revalidate, post-check=0, pre-check=0', 'Expires': 'Thu, 01 Jan 1970 00:00:01 GMT', 'Server': 'cloudflare', 'CF-RAY': '829149a78f32dd2f-LHR', 'alt-svc': 'h3=\":443\"; ma=86400'}\n",
      "Processed batch 59 of 5254\n",
      "Processed batch 60 of 5254\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb Cell 31\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m csv_file \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m../data/wsb_sentiment_results_new2.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39m# Process the DataFrame in batches\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m process_batch(df, batch_size, csv_file)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mProcessing complete.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(start \u001b[39m+\u001b[39m batch_size, \u001b[39mlen\u001b[39m(df))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         batch \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[start:end]\u001b[39m.\u001b[39mcopy()  \u001b[39m# Use .copy() to avoid SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         batch[\u001b[39m'\u001b[39m\u001b[39mSentiment_Tag\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39;49m\u001b[39mText\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m text: throttled_request(\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mYou will be presented with a Reddit message and your job is to provide in return a sentiment tag: choose either the “Positive” OR “Negative” OR “Unclear” tag but NOT several). \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mAssess the sentiment of a Reddit message concerning shares, identified by company names or stock tickers. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mFocus exclusively on sentiments about the shares\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m performance, not the overall business performance. Evaluate the entire post. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mClassify the sentiment as \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPositive\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for actions like buying calls, holding call options, or selling put options; \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39mNegative\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for buying puts, holding put options, or selling call options; \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mand \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mUnclear\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for neutral, mixed, conflicting sentiments, or unclear mentions. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mReturn back a tag with a sentiment concerning the shares. Choose tags ONLY from the list of tags provided above. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mProvide in response just a tag without an explanation.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mMessage: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtext\u001b[39m}\u001b[39;49;00m\u001b[39m\"\"\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m# Append batch results to CSV file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mif\u001b[39;00m start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1077\u001b[0m             values,\n\u001b[1;32m   1078\u001b[0m             f,\n\u001b[1;32m   1079\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1080\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb Cell 31\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         end \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(start \u001b[39m+\u001b[39m batch_size, \u001b[39mlen\u001b[39m(df))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         batch \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[start:end]\u001b[39m.\u001b[39mcopy()  \u001b[39m# Use .copy() to avoid SettingWithCopyWarning\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         batch[\u001b[39m'\u001b[39m\u001b[39mSentiment_Tag\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mText\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: throttled_request(\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39mYou will be presented with a Reddit message and your job is to provide in return a sentiment tag: choose either the “Positive” OR “Negative” OR “Unclear” tag but NOT several). \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mAssess the sentiment of a Reddit message concerning shares, identified by company names or stock tickers. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mFocus exclusively on sentiments about the shares\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m performance, not the overall business performance. Evaluate the entire post. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mClassify the sentiment as \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mPositive\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for actions like buying calls, holding call options, or selling put options; \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39m'\u001b[39;49m\u001b[39mNegative\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for buying puts, holding put options, or selling call options; \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mand \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mUnclear\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m for neutral, mixed, conflicting sentiments, or unclear mentions. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mReturn back a tag with a sentiment concerning the shares. Choose tags ONLY from the list of tags provided above. \u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mProvide in response just a tag without an explanation.\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mMessage: \u001b[39;49m\u001b[39m{\u001b[39;49;00mtext\u001b[39m}\u001b[39;49;00m\u001b[39m\"\"\"\u001b[39;49m))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         \u001b[39m# Append batch results to CSV file\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mif\u001b[39;00m start \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb Cell 31\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mthrottled_request\u001b[39m(text, delay\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     response \u001b[39m=\u001b[39m ask_gpt(text)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     time\u001b[39m.\u001b[39msleep(delay)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m response\n",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb Cell 31\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mask_gpt\u001b[39m(text):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Sending the prompt to the ChatGPT model and getting the response\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         response \u001b[39m=\u001b[39m openai\u001b[39m.\u001b[39;49mChatCompletion\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m             model\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mgpt-3.5-turbo\u001b[39;49m\u001b[39m\"\u001b[39;49m,  \u001b[39m# Adjust the model name as needed\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m             messages\u001b[39m=\u001b[39;49m[{\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39msystem\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mYou are a helpful assistant.\u001b[39;49m\u001b[39m\"\u001b[39;49m}, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m                       {\u001b[39m\"\u001b[39;49m\u001b[39mrole\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mcontent\u001b[39;49m\u001b[39m\"\u001b[39;49m: text}]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# Returning the text of the response\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/02_ChatGPT_negative_upsampling.ipynb#X42sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response\u001b[39m.\u001b[39mchoices[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mmessage[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/openai/api_resources/chat_completion.py:25\u001b[0m, in \u001b[0;36mChatCompletion.create\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/openai/api_resources/abstract/engine_api_resource.py:155\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    131\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[1;32m    139\u001b[0m ):\n\u001b[1;32m    140\u001b[0m     (\n\u001b[1;32m    141\u001b[0m         deployment_id,\n\u001b[1;32m    142\u001b[0m         engine,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    152\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams\n\u001b[1;32m    153\u001b[0m     )\n\u001b[0;32m--> 155\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    156\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mpost\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    157\u001b[0m         url,\n\u001b[1;32m    158\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    159\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    160\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    161\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    162\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[1;32m    166\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n\u001b[1;32m    167\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(response, OpenAIResponse)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/openai/api_requestor.py:289\u001b[0m, in \u001b[0;36mAPIRequestor.request\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    279\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    280\u001b[0m     method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    287\u001b[0m     request_timeout: Optional[Union[\u001b[39mfloat\u001b[39m, Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    288\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Union[OpenAIResponse, Iterator[OpenAIResponse]], \u001b[39mbool\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m--> 289\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_raw(\n\u001b[1;32m    290\u001b[0m         method\u001b[39m.\u001b[39;49mlower(),\n\u001b[1;32m    291\u001b[0m         url,\n\u001b[1;32m    292\u001b[0m         params\u001b[39m=\u001b[39;49mparams,\n\u001b[1;32m    293\u001b[0m         supplied_headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    294\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    295\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    296\u001b[0m         request_id\u001b[39m=\u001b[39;49mrequest_id,\n\u001b[1;32m    297\u001b[0m         request_timeout\u001b[39m=\u001b[39;49mrequest_timeout,\n\u001b[1;32m    298\u001b[0m     )\n\u001b[1;32m    299\u001b[0m     resp, got_stream \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_interpret_response(result, stream)\n\u001b[1;32m    300\u001b[0m     \u001b[39mreturn\u001b[39;00m resp, got_stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapi_key\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/openai/api_requestor.py:606\u001b[0m, in \u001b[0;36mAPIRequestor.request_raw\u001b[0;34m(self, method, url, params, supplied_headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[1;32m    604\u001b[0m     _thread_context\u001b[39m.\u001b[39msession_create_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    605\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 606\u001b[0m     result \u001b[39m=\u001b[39m _thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mrequest(\n\u001b[1;32m    607\u001b[0m         method,\n\u001b[1;32m    608\u001b[0m         abs_url,\n\u001b[1;32m    609\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    610\u001b[0m         data\u001b[39m=\u001b[39;49mdata,\n\u001b[1;32m    611\u001b[0m         files\u001b[39m=\u001b[39;49mfiles,\n\u001b[1;32m    612\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    613\u001b[0m         timeout\u001b[39m=\u001b[39;49mrequest_timeout \u001b[39mif\u001b[39;49;00m request_timeout \u001b[39melse\u001b[39;49;00m TIMEOUT_SECS,\n\u001b[1;32m    614\u001b[0m         proxies\u001b[39m=\u001b[39;49m_thread_context\u001b[39m.\u001b[39;49msession\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    616\u001b[0m \u001b[39mexcept\u001b[39;00m requests\u001b[39m.\u001b[39mexceptions\u001b[39m.\u001b[39mTimeout \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    617\u001b[0m     \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mTimeout(\u001b[39m\"\u001b[39m\u001b[39mRequest timed out: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(e)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/requests/sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    584\u001b[0m send_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m\"\u001b[39m: timeout,\n\u001b[1;32m    586\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mallow_redirects\u001b[39m\u001b[39m\"\u001b[39m: allow_redirects,\n\u001b[1;32m    587\u001b[0m }\n\u001b[1;32m    588\u001b[0m send_kwargs\u001b[39m.\u001b[39mupdate(settings)\n\u001b[0;32m--> 589\u001b[0m resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msend(prep, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49msend_kwargs)\n\u001b[1;32m    591\u001b[0m \u001b[39mreturn\u001b[39;00m resp\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/requests/sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    700\u001b[0m start \u001b[39m=\u001b[39m preferred_clock()\n\u001b[1;32m    702\u001b[0m \u001b[39m# Send the request\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m r \u001b[39m=\u001b[39m adapter\u001b[39m.\u001b[39;49msend(request, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    705\u001b[0m \u001b[39m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[1;32m    706\u001b[0m elapsed \u001b[39m=\u001b[39m preferred_clock() \u001b[39m-\u001b[39m start\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/requests/adapters.py:486\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    483\u001b[0m     timeout \u001b[39m=\u001b[39m TimeoutSauce(connect\u001b[39m=\u001b[39mtimeout, read\u001b[39m=\u001b[39mtimeout)\n\u001b[1;32m    485\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m     resp \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(\n\u001b[1;32m    487\u001b[0m         method\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mmethod,\n\u001b[1;32m    488\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m    489\u001b[0m         body\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mbody,\n\u001b[1;32m    490\u001b[0m         headers\u001b[39m=\u001b[39;49mrequest\u001b[39m.\u001b[39;49mheaders,\n\u001b[1;32m    491\u001b[0m         redirect\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    492\u001b[0m         assert_same_host\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    493\u001b[0m         preload_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    494\u001b[0m         decode_content\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    495\u001b[0m         retries\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    496\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    497\u001b[0m         chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    498\u001b[0m     )\n\u001b[1;32m    500\u001b[0m \u001b[39mexcept\u001b[39;00m (ProtocolError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m    501\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(err, request\u001b[39m=\u001b[39mrequest)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/urllib3/connectionpool.py:714\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    713\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 714\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    715\u001b[0m     conn,\n\u001b[1;32m    716\u001b[0m     method,\n\u001b[1;32m    717\u001b[0m     url,\n\u001b[1;32m    718\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    719\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    720\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    721\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    722\u001b[0m )\n\u001b[1;32m    724\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    725\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    726\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    728\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/urllib3/connectionpool.py:466\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    461\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    462\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 466\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    467\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    468\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/urllib3/connectionpool.py:461\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    460\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 461\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    462\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    463\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    464\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    465\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/http/client.py:1348\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1347\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1348\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1349\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1350\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/http/client.py:316\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    317\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    318\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/http/client.py:277\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 277\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    278\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    279\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to send text to GPT-4 and receive sentiment tag\n",
    "openai.api_key = \"MY_OPENAI_KEY\"\n",
    "\n",
    "def ask_gpt(text):\n",
    "    try:\n",
    "        # Sending the prompt to the ChatGPT model and getting the response\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", \n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": text}]\n",
    "        )\n",
    "        # Returning the text of the response\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def throttled_request(text, delay=1.0):\n",
    "    response = ask_gpt(text)\n",
    "    time.sleep(delay)\n",
    "    return response\n",
    "\n",
    "# Function to process a batch and append results to a CSV file\n",
    "def process_batch(df, batch_size, csv_file):\n",
    "    start = 0\n",
    "    while start < len(df):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch = df.iloc[start:end].copy()\n",
    "        batch['Sentiment_Tag'] = batch['Text'].apply(lambda text: throttled_request(f\"\"\"\n",
    "You will be presented with a Reddit message and your job is to provide in return a sentiment tag: \n",
    "choose either the “Positive” OR “Negative” OR “Unclear” tag but NOT several. \n",
    "Assess the sentiment of a Reddit message concerning shares, identified by company names or stock tickers. \n",
    "Focus exclusively on sentiments about the shares' performance, not the overall business performance. \n",
    "Evaluate the entire post. \n",
    "Classify the sentiment as 'Positive' for actions like buying calls, holding call options, or selling put options; \n",
    "'Negative' for buying puts, holding put options, or selling call options; \n",
    "and 'Unclear' for neutral, mixed, conflicting sentiments, or unclear mentions. \n",
    "Return back a tag with a sentiment concerning the shares. Choose tags ONLY from the list of tags provided above. \n",
    "Provide in response just a tag without an explanation.\n",
    "Message: {text}\"\"\"))\n",
    "        # Append batch results to CSV file\n",
    "        if start == 0:\n",
    "            batch.to_csv(csv_file, mode='w', header=True, index=False)  \n",
    "        else:\n",
    "            batch.to_csv(csv_file, mode='a', header=False, index=False)  \n",
    "        \n",
    "        start += batch_size\n",
    "        print(f\"Processed batch {start // batch_size} of {len(df) // batch_size}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 3  \n",
    "csv_file = '../data/wsb_sentiment_results.csv'\n",
    "\n",
    "# Process the DataFrame in batches\n",
    "process_batch(df, batch_size, csv_file)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6. GPT-4 repeat labelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3648, 8)\n"
     ]
    }
   ],
   "source": [
    "# Read in data saved in a csv file in the previoius step\n",
    "file_path = '../data/wsb_sentiment_results.csv'\n",
    "df_check = pd.read_csv(file_path)\n",
    "\n",
    "print(df_check.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a resut of partial processing of the Reddit unlabelled messages with Chat GPT 3.5 API I have accumulated 3648 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>Sentiment_Tag</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mqp6lv</td>\n",
       "      <td>COIN IPO Megathread 4/14/2021</td>\n",
       "      <td>This is a megathread for the $COIN IPO.  We will allow discussion of the COIN stock, but do note that the no crypto rule still applies.  If you use COIN as a proxy to discuss crypto, instead of discussing the stock, you will be banned.  Itâs a bit of a gray line, so ensure that you keep discussion focused on the stock.\\n\\n\\nEdit: Direct Listing*</td>\n",
       "      <td>COIN IPO Megathread 4/14/2021, This is a megathread for the $COIN IPO.  We will allow discussion of the COIN stock, but do note that the no crypto rule still applies.  If you use COIN as a proxy to discuss crypto, instead of discussing the stock, you will be banned.  Itâs a bit of a gray line, so ensure that you keep discussion focused on the stock.\\n\\n\\nEdit: Direct Listing*</td>\n",
       "      <td>103</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pk8tne</td>\n",
       "      <td>For anyone who doesnât understand why Hedgefonds lost, this ape explained it well.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>For anyone who doesnât understand why Hedgefonds lost, this ape explained it well., nan</td>\n",
       "      <td>21</td>\n",
       "      <td>Unclear</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  \\\n",
       "0  mqp6lv   \n",
       "1  pk8tne   \n",
       "\n",
       "                                                                                  title  \\\n",
       "0                                                         COIN IPO Megathread 4/14/2021   \n",
       "1  For anyone who doesnât understand why Hedgefonds lost, this ape explained it well.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                        selftext  \\\n",
       "0  This is a megathread for the $COIN IPO.  We will allow discussion of the COIN stock, but do note that the no crypto rule still applies.  If you use COIN as a proxy to discuss crypto, instead of discussing the stock, you will be banned.  Itâs a bit of a gray line, so ensure that you keep discussion focused on the stock.\\n\\n\\nEdit: Direct Listing*   \n",
       "1                                                                                                                                                                                                                                                                                                                                                            NaN   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                           Text  \\\n",
       "0  COIN IPO Megathread 4/14/2021, This is a megathread for the $COIN IPO.  We will allow discussion of the COIN stock, but do note that the no crypto rule still applies.  If you use COIN as a proxy to discuss crypto, instead of discussing the stock, you will be banned.  Itâs a bit of a gray line, so ensure that you keep discussion focused on the stock.\\n\\n\\nEdit: Direct Listing*   \n",
       "1                                                                                                                                                                                                                                                                                                     For anyone who doesnât understand why Hedgefonds lost, this ape explained it well., nan   \n",
       "\n",
       "   token_count Sentiment_Tag  Unnamed: 6 Unnamed: 7  \n",
       "0          103       Unclear         NaN        NaN  \n",
       "1           21       Unclear         NaN        NaN  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set maximum column width to None to display full text content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df_check.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment_Tag\n",
      "Unclear     1545\n",
      "Positive    1171\n",
      "Negative     725\n",
      "Error        203\n",
      "Neutral        4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Getting frequency counts for 'Sentiment_Tag'\n",
    "tag_counts = df_check['Sentiment_Tag'].value_counts()\n",
    "\n",
    "# Displaying the frequency counts\n",
    "print(tag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(725, 8)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check = df_check[df_check['Sentiment_Tag'] != 'Error'].copy()\n",
    "df_check = df_check[df_check['Sentiment_Tag'] != 'Unclear'].copy()\n",
    "df_check = df_check[df_check['Sentiment_Tag'] != 'Positive'].copy()\n",
    "df_check = df_check[df_check['Sentiment_Tag'] != 'Neutral'].copy()\n",
    "df_check.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In total, I have 725 negative ratings, or 20% of the data given to GPT 3.5 (first \"set of eyes\"). This percentage seems to be reasonable and consistent with my expectations. \n",
    "\n",
    "When I review the quality of the GPT 3.5 ratings, I notice that while many of them are correct, not all of them are perfectly on point.  Some ratings I disagree with. In order to increase my confidence in having good quality ratings, I decided to screen them again, using a different GPT model (GPT 4). \n",
    "\n",
    "In the code below I am re-checking the negative ratings from the previous step with GPT-4, a more advanced model (also more expensive to process tokens). I essentially treat GPT-4 model as a second set of eyes (as another human) to confirm the ratings given to me by Chat GPT 3.5 model.  I record the results of GPT-4 model processing in a new column Sentiment_Tag_2, in batches, in a new csv file. \n",
    "\n",
    "For my negative ratings up sampling I will select only those that were rated as \"negative\" by both models. \n",
    "\n",
    "The code below is a replica of the code used in teh previous section, but with GPT-4 model, not GPT 3.5 as before: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1 of 4\n",
      "Processed batch 2 of 4\n",
      "Processed batch 3 of 4\n",
      "Processed batch 4 of 4\n",
      "Processed batch 5 of 4\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Function to send text to GPT-4 and receive sentiment tag (you need to implement this)\n",
    "openai.api_key = \"MY_OPENAI_KEY\"\n",
    "\n",
    "def ask_gpt(text):\n",
    "    try:\n",
    "        # Sending the prompt to the ChatGPT model and getting the response\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  \n",
    "            messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, \n",
    "                      {\"role\": \"user\", \"content\": text}]\n",
    "        )\n",
    "        # Returning the text of the response\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return \"Error\"\n",
    "\n",
    "def throttled_request(text, delay=5.0):\n",
    "    response = ask_gpt(text)\n",
    "    time.sleep(delay)\n",
    "    return response\n",
    "\n",
    "# Function to process a batch and append results to a CSV file\n",
    "def process_batch(df, batch_size, csv_file):\n",
    "    start = 0\n",
    "    while start < len(df):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        batch = df.iloc[start:end].copy()  \n",
    "        batch['Sentiment_Tag_2'] = batch['Text'].apply(lambda text: throttled_request(f\"\"\"You will be presented with a Reddit message and your job is to provide in return a sentiment tag: choose either the “Positive” OR “Negative” OR “Unclear” tag but NOT several). \n",
    "Assess the sentiment of a Reddit message concerning shares, identified by company names or stock tickers. \n",
    "Focus exclusively on sentiments about the shares' performance, not the overall business performance. Evaluate the entire post. \n",
    "Classify the sentiment as 'Positive' for actions like buying calls, holding call options, or selling put options; \n",
    "'Negative' for buying puts, holding put options, or selling call options; \n",
    "and 'Unclear' for neutral, mixed, conflicting sentiments, or unclear mentions. \n",
    "Return back a tag with a sentiment concerning the shares. Choose tags ONLY from the list of tags provided above. \n",
    "Provide in response just a tag without an explanation.\n",
    "Message: {text}\"\"\"))\n",
    "        # Append batch results to CSV file\n",
    "        if start == 0:\n",
    "            batch.to_csv(csv_file, mode='w', header=True, index=False)  \n",
    "        else:\n",
    "            batch.to_csv(csv_file, mode='a', header=False, index=False)  \n",
    "        \n",
    "        start += batch_size\n",
    "        print(f\"Processed batch {start // batch_size} of {len(df) // batch_size}\")\n",
    "\n",
    "# Parameters\n",
    "batch_size = 150  \n",
    "csv_file = '../data/wsb_sentiment_results_check.csv'\n",
    "\n",
    "# Process the DataFrame in batches\n",
    "process_batch(df_check, batch_size, csv_file)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7. Re-checking the chat GPT-4 ratings and comparing two GPT model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(725, 7)\n"
     ]
    }
   ],
   "source": [
    "# Read in data\n",
    "df_check_gpt4 = pd.read_csv('../data/wsb_sentiment_results_check.csv')\n",
    "print(df_check_gpt4.shape)\n",
    "# df = df.head(9)\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 725 entries, 0 to 724\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   id               725 non-null    object\n",
      " 1   title            725 non-null    object\n",
      " 2   selftext         464 non-null    object\n",
      " 3   Text             725 non-null    object\n",
      " 4   token_count      725 non-null    int64 \n",
      " 5   Sentiment_Tag    725 non-null    object\n",
      " 6   Sentiment_Tag_2  725 non-null    object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 39.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df_check_gpt4.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having a look at the new ratings (see new column Sentriment_Tag_2),  anc comparing the output of two GPT models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>Sentiment_Tag</th>\n",
       "      <th>Sentiment_Tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>m0iyjz</td>\n",
       "      <td>$RKT Puts</td>\n",
       "      <td>Today my puts randomly changed prices. Is this...</td>\n",
       "      <td>$RKT Puts, Today my puts randomly changed pric...</td>\n",
       "      <td>28</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>mq7yi5</td>\n",
       "      <td>SPY puts</td>\n",
       "      <td>[spy](https://i.postimg.cc/j20LdVxJ/FEF3-A80-B...</td>\n",
       "      <td>SPY puts, [spy](https://i.postimg.cc/j20LdVxJ/...</td>\n",
       "      <td>150</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>mqe8ox</td>\n",
       "      <td>XL Puts 4/16</td>\n",
       "      <td>I know its very risky but i feel like its a sa...</td>\n",
       "      <td>XL Puts 4/16, I know its very risky but i feel...</td>\n",
       "      <td>29</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>560</th>\n",
       "      <td>nk6jum</td>\n",
       "      <td>They refuse to let GME close above $180. WTH</td>\n",
       "      <td>Where are they getting all these shares to sho...</td>\n",
       "      <td>They refuse to let GME close above $180. WTH, ...</td>\n",
       "      <td>152</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>mlp05i</td>\n",
       "      <td>She looks hungry for some shorts tomorrow 🍌🚀🤨 ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>She looks hungry for some shorts tomorrow 🍌🚀🤨 ...</td>\n",
       "      <td>21</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "402  m0iyjz                                          $RKT Puts   \n",
       "370  mq7yi5                                           SPY puts   \n",
       "377  mqe8ox                                       XL Puts 4/16   \n",
       "560  nk6jum       They refuse to let GME close above $180. WTH   \n",
       "701  mlp05i  She looks hungry for some shorts tomorrow 🍌🚀🤨 ...   \n",
       "\n",
       "                                              selftext  \\\n",
       "402  Today my puts randomly changed prices. Is this...   \n",
       "370  [spy](https://i.postimg.cc/j20LdVxJ/FEF3-A80-B...   \n",
       "377  I know its very risky but i feel like its a sa...   \n",
       "560  Where are they getting all these shares to sho...   \n",
       "701                                                NaN   \n",
       "\n",
       "                                                  Text  token_count  \\\n",
       "402  $RKT Puts, Today my puts randomly changed pric...           28   \n",
       "370  SPY puts, [spy](https://i.postimg.cc/j20LdVxJ/...          150   \n",
       "377  XL Puts 4/16, I know its very risky but i feel...           29   \n",
       "560  They refuse to let GME close above $180. WTH, ...          152   \n",
       "701  She looks hungry for some shorts tomorrow 🍌🚀🤨 ...           21   \n",
       "\n",
       "    Sentiment_Tag Sentiment_Tag_2  \n",
       "402      Negative        Negative  \n",
       "370      Negative        Negative  \n",
       "377      Negative        Negative  \n",
       "560      Negative        Positive  \n",
       "701      Negative        Positive  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_gpt4.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment_Tag_2\n",
      "Negative    450\n",
      "Unclear     208\n",
      "Positive     67\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(725, 7)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting frequency counts for 'Sentiment_Tag'\n",
    "tag_counts = df_check_gpt4['Sentiment_Tag_2'].value_counts()\n",
    "\n",
    "# Displaying the frequency counts\n",
    "print(tag_counts)\n",
    "df_check_gpt4.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a considerable degree of disagreement between the two GPT models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a crosstab of the two columns\n",
    "crosstab_result = pd.crosstab(df_check_gpt4['Sentiment_Tag'], df_check_gpt4['Sentiment_Tag_2'])\n",
    "\n",
    "# Plotting the crosstab\n",
    "plt.figure(figsize=(4, 2))  \n",
    "sns.heatmap(crosstab_result, annot=True, fmt=\"d\", cmap=\"coolwarm\")  # 'fmt' is format of the annotation (integer in this case)\n",
    "\n",
    "# Adding titles and labels\n",
    "plt.title('Crosstab of GPT 4 vs GPT 3.5')\n",
    "plt.xlabel('Sentiment GPT 4')\n",
    "plt.ylabel('Sentiment GPT 3.5')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "450 rows weer rated as \"negative\" by both GPT models.  This is 62% of the starting number of rows rated as negative by Chat GPT 3.5 (725 rows).  In 9% of cases Chat GPT 4 completely disagreed with the negative rating given by Chat GPT 3.5 and assigned \"positive\" rating to the text. \n",
    "\n",
    "**Conclusion**:  I will treat those instances where both Chat GPT models agreed that the rating was \"negative\" as the additional \"ground truth\" for my model training.  This will help to address the deficit of negative ratings in the labelled dataset and will help in training my models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(450, 7)\n"
     ]
    }
   ],
   "source": [
    "# Deleting rows where two GPT models disagree with each other. Keeping only those rows where both ratings are \"negative\"\n",
    "df_check_gpt4 = df_check_gpt4[df_check_gpt4['Sentiment_Tag_2'] != 'Unclear'].copy()\n",
    "df_check_gpt4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_check_gpt4 = df_check_gpt4[df_check_gpt4['Sentiment_Tag_2'] != 'Positive'].copy()\n",
    "df_check_gpt4.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_check_gpt4.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8. Cleaning the newly generated negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below I repeat the same cleaning steps as in the notbook 01 Labelled EDA and Cleaning, to ensure pre-processing consistency: \n",
    "\n",
    "* checking for missing values and duplicates\n",
    "* removing website inks (urls), hashtags (#) and mentions (@)\n",
    "* the resulting dataset is then saved into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>Sentiment_Tag</th>\n",
       "      <th>Sentiment_Tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, selftext, Text, token_count, Sentiment_Tag, Sentiment_Tag_2]\n",
       "Index: []"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_gpt4[ df_check_gpt4['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>Sentiment_Tag</th>\n",
       "      <th>Sentiment_Tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, title, selftext, Text, token_count, Sentiment_Tag, Sentiment_Tag_2]\n",
       "Index: []"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_check_gpt4[df_check_gpt4['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df_check_gpt4.copy()\n",
    "df_clean.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def purge_content(text):\n",
    "    text_without_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text_without_hashtags = re.sub(r'#\\S+', '', text_without_urls)\n",
    "    text_without_mentions = re.sub(r'@\\S+', '', text_without_hashtags)\n",
    "    text_without_linebreaks = re.sub(r'\\n+', '', text_without_mentions)\n",
    "    clean_text = re.sub(r'\\n+', ' ', text_without_linebreaks)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using apply() method to clean the text\n",
    "df_clean['Text'] = df_clean['Text'].apply(purge_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>Text</th>\n",
       "      <th>token_count</th>\n",
       "      <th>Sentiment_Tag</th>\n",
       "      <th>Sentiment_Tag_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>m2jt64</td>\n",
       "      <td>To my best friend who experienced his first loss</td>\n",
       "      <td>(Disclaimer: This is my humble opinion about t...</td>\n",
       "      <td>To my best friend who experienced his first lo...</td>\n",
       "      <td>1061</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>nchd2q</td>\n",
       "      <td>I promised you an update after earnings... And...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I promised you an update after earnings... And...</td>\n",
       "      <td>64</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>r0sqxy</td>\n",
       "      <td>PLTR CEO on CNBC convinced me that I need to b...</td>\n",
       "      <td>I have been using PLTR for the wheel strategy ...</td>\n",
       "      <td>PLTR CEO on CNBC convinced me that I need to b...</td>\n",
       "      <td>233</td>\n",
       "      <td>Negative</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              title  \\\n",
       "221  m2jt64   To my best friend who experienced his first loss   \n",
       "128  nchd2q  I promised you an update after earnings... And...   \n",
       "141  r0sqxy  PLTR CEO on CNBC convinced me that I need to b...   \n",
       "\n",
       "                                              selftext  \\\n",
       "221  (Disclaimer: This is my humble opinion about t...   \n",
       "128                                                NaN   \n",
       "141  I have been using PLTR for the wheel strategy ...   \n",
       "\n",
       "                                                  Text  token_count  \\\n",
       "221  To my best friend who experienced his first lo...         1061   \n",
       "128  I promised you an update after earnings... And...           64   \n",
       "141  PLTR CEO on CNBC convinced me that I need to b...          233   \n",
       "\n",
       "    Sentiment_Tag Sentiment_Tag_2  \n",
       "221      Negative        Negative  \n",
       "128      Negative        Negative  \n",
       "141      Negative        Negative  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9. Saving the cleaned dataframe into a CSV file for further steps in modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df_clean.to_csv('../data/reddit_gpt4_negative_only_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.10. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have identified 450 additional negative intentions messages.  \n",
    "* 3.6 thousands messages fed to two GPT models yielded 450 double labelled negative messages. \n",
    "* Total cost - around $12.  \n",
    "* The main limiting factor was time, as GPT 3.5 API is very slow and prone to freezing up and crashing.  \n",
    "\n",
    "I will use the newly labelled messages to upsample my imbalaned labelled dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
