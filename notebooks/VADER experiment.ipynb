{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lgfolder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing several libraries\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6.0, 4.0) #setting figure size\n",
    "import seaborn as sns\n",
    "import matplotlib.dates as mdates\n",
    "import os\n",
    "import emoji\n",
    "import re\n",
    "pd.set_option('display.float_format', '{:.2f}'.format) # Set the float display option\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = '../data/reddit_cleaned.csv'\n",
    "df_all = pd.read_csv(DATAFILE)\n",
    "df = df_all.head(50000).copy()\n",
    "# df = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   50000 non-null  object \n",
      " 1   author               50000 non-null  object \n",
      " 2   created              50000 non-null  object \n",
      " 3   removed              50000 non-null  int64  \n",
      " 4   deleted              50000 non-null  int64  \n",
      " 5   is_self              50000 non-null  int64  \n",
      " 6   is_video             50000 non-null  int64  \n",
      " 7   title                50000 non-null  object \n",
      " 8   link_flair_text      50000 non-null  object \n",
      " 9   upvote_ratio         50000 non-null  float64\n",
      " 10  score                50000 non-null  int64  \n",
      " 11  num_comments         50000 non-null  int64  \n",
      " 12  selftext             29228 non-null  object \n",
      " 13  shortlink            50000 non-null  object \n",
      " 14  FolderName           50000 non-null  object \n",
      " 15  word_count_selftext  50000 non-null  int64  \n",
      " 16  word_count_title     50000 non-null  int64  \n",
      " 17  date                 50000 non-null  object \n",
      "dtypes: float64(1), int64(8), object(9)\n",
      "memory usage: 6.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['Unnamed: 0','Unnamed: 0.1'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext'].fillna('notextprovided', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing value: id                     0\n",
      "author                 0\n",
      "created                0\n",
      "removed                0\n",
      "deleted                0\n",
      "is_self                0\n",
      "is_video               0\n",
      "title                  0\n",
      "link_flair_text        0\n",
      "upvote_ratio           0\n",
      "score                  0\n",
      "num_comments           0\n",
      "selftext               0\n",
      "shortlink              0\n",
      "FolderName             0\n",
      "word_count_selftext    0\n",
      "word_count_title       0\n",
      "date                   0\n",
      "dtype: int64\n",
      "Duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check data quality\n",
    "print(f\"Missing value: {df.isna().sum()}\")\n",
    "print(f\"Duplicated rows: {df.duplicated().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selftext\n",
       "notextprovided                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       20772\n",
       "[removed]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             4152\n",
       "[deleted]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             1658\n",
       " \\n\\nTalk about your plays today or things you are on the lookout for. This is where you belong if your comment includes a ticker.\\n\\n*keep it civil please*                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            79\n",
       "In an effort to keep the \"main sub\" a little cleaner in regards to \"low effort posts,\" this will be a catch all for the simple questions that get asked on a regular basis.\\n\\nWe want to help new investors/traders. That's definitely one of the main goals of this community. We don't want to run people off, but at the same time we want there to at least be some sort of standard to what constitutes \"low effort.\" We wish to differentiate between legitimate, detailed questions predicated upon at least a base level of due dilligence and questions that may be better served going into an \"other folder,\" so to speak.\\n\\nAlso to note, anything that fits the description of what goes in this thread will be deleted from the \"main sub,\" so there may be a learning curve of people wondering why their posts are getting deleted. This new format sticky thread will be auto re-posted daily so as not to get too cluttered.\\n\\nThe following is a list of what is relegated to this \"catch all\" thread, and is subject to change based on the needs of the sub:\\n\\n1. What broker should I use?\\n2. What do you guys think of \"XXXX\" stock?\\n3. Should I buy or sell \"XXXX\" stock?\\n4. Any threads with ZERO DD\\n5. Anything that would have gone into the \"any stocks go here\" sticky thread (cryprocurrency is still banned)\\n6. Any questions you think might be \"stupid\" questions\\n7. Any post requesting people's thoughts on your thoughts                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  58\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ...  \n",
       "I've been following this subreddit and a few others since last year looking for new, interesting companies or investment opportunities and I have to pretty much all the picks I've seen pushed here are in a worse position now and have been for a while.\\n\\nI'm not really trying to insinuate anything but curious on other people's experience on Reddit stock recommendations as they now account for literally all of my poor positions.\\n\\nInterested to hear people's (especially lurkers) thoughts...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1\n",
       "Ever since the announcement of Omicron last week, markets have been experiencing more volatility. At first, it seemed scary on Thanksgiving when news break of the high number of mutations. but now it's turning out to be a milder version of Delta. Us retail investors are thinking, \"why panic over a COVID variant now all of a sudden?\". Delta wasn't a big deal, why do markets care now?\\n\\nMaybe it's because Omicron presents the idea that COVID is officially over and it'll now be similar to a endemic flu. Omicron is turning out to have very mild symptoms while being highly transmissible. This may be the variant we've been waiting for to make COVID-19 a thing of the past. As we know, the FED has had a huge impact on markets the past few years. Omicron signals that the FED no longer has an excuse for QE or money printing. Markets are forward-looking and maybe investors view this as the final block to closing up the COVID-19 chapter in 2022. Now tapering and raising rates are coming closer and closer to reality...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           1\n",
       "https://www.youtube.com/watch?v=VwXLRoAw3Z4\\n\\nThese motherfuckers are slippery.\\n\\nIn short, Hedgies might be short a million. So they go and buy 2 million worth of ITM contracts at 60 dollars. 1 million to clear their short positions and the other million.... HEY PRESTO, TO THE MOON DIAMOND HANDS! They just became Apes who flipped from short positions to long positions. **They now want GME to moon.**\\n\\nIt's like Uncle Melvin has changed fucking sides.\\n\\nAnd the MM option writers are in line for severe anal abuse, because they gotta start finding shares. Millions of shares. To deal with the ITM calls/naked calls. \\n\\nUncle Melvin flipped sides and now there's a new target for him - MM Contract writers. If this is true, Melvin literally just did a perfect WW2 Italy move. He saw who the winning side is and just went \"mama mia, I joina you!\"\\n\\nwhat I can see here is that this is a way for Melvin to EXIT short positions, STILL FUCKING MAKE MONEY and also prevent the parabolic shortsqueeze, with essentially a gamma style squeeze with the catalyst being the purchases being made by contract writers.\\n\\nWhen it settles down... BOOM, MELVIN PLAYS ANOTHER ITALY and he switches sides again, and starts shorting GME on the *way back down*. Rinse and fucking repeat until it all fizzles out.\\n\\nMonday is gonna be EXCITING AS FUCK.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            1\n",
       "I've been holding since Jan @ 315 avg and one thing I noticed time after time is that legit any hype days never came to fruition. It was the days where everybody was kinda chilling and then ***boom***,  a huge load drops from the heavens above and we ascend to the hundreds (recall that beautiful Wednesday when the rocket started to b l a s t of f). \\n\\n**Looking back...**\\n\\n* When the FINRA data came out (multiple occasions)\\n* When DFV went on congress \\n* When DFV DOUBLED DOWN (albeit shit happened a few days after) \\n* When we expected that one Friday to pop off because of millions of calls. Literally all those calls went poof into the air cuz they expired since they weren't ITM. I think that was Feb 26. \\n\\nSo many of those hype days were just kinda boring-watching-the-paint-dry-diamond-hand-strengthening days. NOTHING HAPPENED ***and that's okay, because look at where we are now.*** \\n\\nToday was also another day. After that massive drop yesterday that got us on the SSR list, we thought there would be some dope shit happening - I think it's unhealthy to hype each other up like this because at the end of the day, we're just on the schedule of the Whales/HFs. Whenever they make a move is whenever that \\*ACTION\\* is going to happen. \\n\\nTo the stronger people out there, this isn't phasing us that much or at all. But consider others who may not be as strong - all this hype around an \"amazing\" day and then leading to a \"normal\" day is demoralizing. It's like being promised an amazing deluxe ice cream fudge sundae with all the toppings in the world and then being handed a 🍦 from McDonalds. On a normal day, you'd be happy with that damn soft serve, but now you're angry/upset/frustrated because you were expecting something better.\\n\\nUs ending at $260 is amazing today (plus the volume was the lowest it was this week!). To the newcomers, imagine it was trading flat every day around $40-50 for 1+ month. The fact we're in the 200s IS AMAZING. But to add on - like the apes with wrinkly brains (e.g. /u/rensole ) out there are saying, STOP PUTTING A DAMN DATE ON SHIT. That includes March 19th. STOP IT. \\n\\nThat being said though, I'm not saying don't hype things up. Instead, still HYPE IT UP but also be like ***\"and if this doesn't happen, then we'll just wait because the DD supports us blasting off to space and I don't mind waiting a few more days/weeks/\\[length of time\\] to be a millionaire :)*** \\n\\nSEE YOU IN SPACE YOU APES 🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍🦍        1\n",
       " .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       1\n",
       "Name: count, Length: 22863, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check class counts\n",
    "df['selftext'].value_counts() \n",
    "# df.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Removing moderators ??? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Spam messages - shall I remove them? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. TextBlob sentiment score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.X. VADER sentiment score calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an object of SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def calculate_sentiment(text):\n",
    "    if pd.isnull(text):\n",
    "        return {'neg': None, 'neu': None, 'pos': None, 'compound': None}\n",
    "    \n",
    "    # Analyze the sentiment of the post\n",
    "    sentiment_score = analyzer.polarity_scores(text)\n",
    "    return sentiment_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeyItsPixeL\n",
      "Hi everyone. I am gonna be honest with you, I am really starting to get pissed about people spreading lies about what I said. I am getting tagged in about 150 - 200 posts a day and about the same amount of comments. Half of which is people trying to slander my name and telling other people lies about what I said is going to happen this week and how I supposedly said, this is \"the last chance\" (quote by one of the dumbf\\*\\*\\* that slandered me over the last few weeks) for a Gamma Squeeze or Short Squeeze to happen.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I am calling out this post specifically, because it gained the most traction and called out u/rensole and me:\n",
      "\n",
      "[https:\\/\\/www.reddit.com\\/r\\/GME\\/comments\\/m5spf5\\/reminder\\_that\\_the\\_19th\\_is\\_not\\_the\\_last\\_possible\\/](https://preview.redd.it/qa1ogy259cn61.png?width=740&format=png&auto=webp&s=b479bb5b30085e2cf5c2ee345105dbb3ccdb9d18)\n",
      "\n",
      "(Longer TL;DR at point III)\n",
      "\n",
      "TL;DR: And I just want to clarify, because it actually seems to be a new FUD campaign (what isn't at this point lol), that theyTM want to do, is split people as hard as they can. (nothing new, but it's just really obvious this week) Every few minutes there is a new post about the importance of this week and then there is the same amount of people saying fuck this week. My thoughts? **I stand by my Endgame DD BUT(!!!!!) I really don't care what happens this week**. Of course I want my prediction to become true, but if it doesn't? I'll keep on living and hodl urge you to do the same. There are so many catalysts this week, next week (even more for next week as of now to be honest, but they weren't there when I created the Endgame DD) and the weeks after. I will just wait and see and can only urge people to do the same.\n",
      "\n",
      "Quoting my own comment:\n",
      "\n",
      "[https:\\/\\/www.reddit.com\\/r\\/wallstreetbets\\/comments\\/m5jn1y\\/gme\\_megathread\\_for\\_march\\_15\\_2021\\/gr1xnl9?utm\\_source=share&utm\\_medium=web2x&context=3](https://preview.redd.it/09o7j3xp3cn61.png?width=683&format=png&auto=webp&s=9ca1d4d157fe84af469c3cc5088bb6a3ec3880d9)\n",
      "\n",
      "So please. Don't follow every bullshit post calling me out and be critical, here is why.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "I. I created my Endgame DD 16 days ago. I stated that this is **MY** predicted date for the **START** of a squeeze. The bullshit post I quoted above stated the following:\n",
      "\n",
      "https://preview.redd.it/zyy0xkq25cn61.png?width=699&format=png&auto=webp&s=bd38003151a250c6cd0aa530ec842b8318dff030\n",
      "\n",
      "So far so legitimate, of course that can be his opinion. But who are persons behind such posts? Now it's just getting stupid and that's the thing that drives me mad the most.\n",
      "\n",
      "https://preview.redd.it/svn06nhr5cn61.png?width=741&format=png&auto=webp&s=b59cfb2f343656167312d141eaeb3f576a779a7a\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "II. **Persons behind posts like that**: They are almost always (85 - 90 %) either new accounts, revived accounts (dead for years/months and suddenly start posting on GME, etc.) or they are just inexperienced people that have no clue what they are talking about.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "In this case we are looking at point 3 \"inexperienced people that have no clue what they are talking about\". The guy that called me out jumped on the GME bandwagon just 3 weeks ago. Before that, he didn't have a single post about trading on his whole profile. Doesn't mean anything, could just be a closet trader, right? Look at his comments over the past 1 - 3 weeks.\n",
      "\n",
      "https://preview.redd.it/pxpcxcuh6cn61.png?width=378&format=png&auto=webp&s=685af0d9e5f498934af271af7f1010443377c43f\n",
      "\n",
      "https://preview.redd.it/xpg11euh6cn61.png?width=315&format=png&auto=webp&s=6f92ec68b837c1864cf12e24d0808ab0be33b8ea\n",
      "\n",
      "https://preview.redd.it/jd743euh6cn61.png?width=516&format=png&auto=webp&s=7619020cd0a16e34adebdcaa15a0f541c9335ca7\n",
      "\n",
      "https://preview.redd.it/4tujueuh6cn61.png?width=243&format=png&auto=webp&s=4c15baaf7ee829eed483c06401271e4805cd7f63\n",
      "\n",
      "https://preview.redd.it/52o93guh6cn61.png?width=250&format=png&auto=webp&s=0e71c63c9dd6d2439b5bb380b8cf1e20161a1b26\n",
      "\n",
      "https://preview.redd.it/zn8v0fuh6cn61.png?width=425&format=png&auto=webp&s=01cd41ff5759c14ab045e14e2715c73bb9814426\n",
      "\n",
      "https://preview.redd.it/4me09fuh6cn61.png?width=495&format=png&auto=webp&s=ac1973c8b7d72ae07dc0acf6fbb4d1afdf8e9ef9\n",
      "\n",
      "I am gonna spare you with more, just look at his profile. It literally only consits of such comments.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "**Conclusion:** Someone who has no experience in trading, being an asshole to the community for weeks, asking for selling points, spreading FUD over a month, saying the squeeze is over and he is going to sell is suddenly an expert on all this and wants people to stop putting dates on things, because it's spreading FUD? BRUUUUH. Don't get me wrong, you don't have to be an expert to give your opinion on stuff, but calling me out like that without any substance and telling people lies about what I said is going to happen this week? That's just bullshit. The person that posted this is a paperhanded pussy, dividing the community with FUD and lies and wants people to agree with him, because he doesn't has the nerve to hold through these dips himself. Especially the OP of the post stated above, wants to make himself look like he holds the community together now, telling people to hold, etc.. He literally spread FUD over one whole month and now this? Give me a break you hypocrite. u/qwerksd\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "III. **Clarification about my Endgame DD, Dates, this week, what the guy above stated I apparently said, etc.:** I NEVER EVER(!) said this week is going to be the last chance for a squeeze (Gamma/Short). I said, data supports, that it's going to start at AROUND march 19th. Maybe this week, maybe next week, who the fuck knows? People really just need to hold and stop it with the bullshit posts trying to divide people. That's why I stepped down from posting at the moment. I don't want to add fuel to the fire, by giving such people more opportunities to badmouth and lie about what I said. They damn well know, that I can't respond to every comment because of the ammount of comments, messages, tags etc. I receive everyday. So of course it looks like I don't reply to critique. People that know me a bit better by now know, I love doing that.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "Just wanted to clarify all this. Maybe it makes up peoples mind a bit that this is not a \"do or die moment\" for GME. **This is a \"would be nice if this happens\" moment.** The amount of catalysts and data just supports, that things COULD happen in the near near future. But as I just said 20 times, be patient and hodl.\n",
      "\n",
      "**My opinion on all this:** People saying we should stop to put date on things, are just paper handed pussies and want people to agree, so they won't get dissapointed and paperhand out, when nothing happens on dates being talked about. If there are legitimate reasons to talk about dates and there are dates, on which big things could happen? TALK ABOUT THEM. You don't have to say things will become true, you don't even have to put your opinion out there, but no one should ban you from talking about dates, just because they have nerves made out of used toilet paper. If people hate it, they downvote it. If people like it, they upvote it and spread the word, because there really seems to be good reasoning and reasearch behind such a post. Stop telling people what to do and what not to do, there are functions in reddit for that.\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "**IN SHORT: I LIKE THE STOCK 💎🙌 And no matter what happens this week, next week, next months, this year, I don't give a fuck. I am going to hold and be patient. This is not a do or die moment. This is a \"would be nice if this happens\" moment.**\n",
      "\n",
      "&#x200B;\n",
      "\n",
      "*No financial advice, I eat crayons for a living, karen please call me i still want to see the kids.*\n",
      "{'neg': 0.103, 'neu': 0.808, 'pos': 0.09, 'compound': -0.9759}\n"
     ]
    }
   ],
   "source": [
    "# This code checks the content of each inidividual text cell \n",
    "i = 1012\n",
    "print(df[\"author\"].loc[i])\n",
    "print(df[\"selftext\"].loc[i])\n",
    "post = df[\"selftext\"].loc[i]\n",
    "\n",
    "# Calculates the sentiment scores for the chosen cell\n",
    "overall_sentiment_score = analyzer.polarity_scores(post)\n",
    "print(overall_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=50000, step=1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculate_sentiment function to each row in the 'title' column (runtime = 2 seconds)\n",
    "sentiments = df['title'].apply(calculate_sentiment)\n",
    "\n",
    "# Record the sentiments into the dataframe (ttl=title, v= vader)\n",
    "df.loc[:, 'neg_ttl_v'] = sentiments.apply(lambda x: x['neg'])\n",
    "df.loc[:, 'neu_ttl_v'] = sentiments.apply(lambda x: x['neu'])\n",
    "df.loc[:, 'pos_ttl_v'] = sentiments.apply(lambda x: x['pos'])\n",
    "df.loc[:, 'compound_ttl_v'] = sentiments.apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the calculate_sentiment function to each row in the 'selftext' column (runtime = 3 min+)\n",
    "sentiments = df['selftext'].apply(calculate_sentiment)\n",
    "\n",
    "# Record the sentiments into the dataframe (ttl=title, v= vader)\n",
    "df.loc[:, 'neg_st_v'] = sentiments.apply(lambda x: x['neg'])\n",
    "df.loc[:, 'neu_st_v'] = sentiments.apply(lambda x: x['neu'])\n",
    "df.loc[:, 'pos_st_v'] = sentiments.apply(lambda x: x['pos'])\n",
    "df.loc[:, 'compound_st_v'] = sentiments.apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>created</th>\n",
       "      <th>removed</th>\n",
       "      <th>deleted</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>title</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>date</th>\n",
       "      <th>neg_ttl_v</th>\n",
       "      <th>neu_ttl_v</th>\n",
       "      <th>pos_ttl_v</th>\n",
       "      <th>compound_ttl_v</th>\n",
       "      <th>neg_st_v</th>\n",
       "      <th>neu_st_v</th>\n",
       "      <th>pos_st_v</th>\n",
       "      <th>compound_st_v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12125</th>\n",
       "      <td>rssid0</td>\n",
       "      <td>NoMoneyNoMe</td>\n",
       "      <td>2021-12-31 12:04:49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>How to profit from the upcoming war between NA...</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>0.52</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40609</th>\n",
       "      <td>lcbahe</td>\n",
       "      <td>Suicidal_Donut</td>\n",
       "      <td>2021-02-04 08:30:59</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What do you think people will do?</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>0.82</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-02-04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.03</td>\n",
       "      <td>-0.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10783</th>\n",
       "      <td>rpulu4</td>\n",
       "      <td>_STIFFL3R_</td>\n",
       "      <td>2021-12-27 19:03:39</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Marry xmas and a happy new year!</td>\n",
       "      <td>Loss</td>\n",
       "      <td>0.96</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-12-27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19135</th>\n",
       "      <td>l7xjio</td>\n",
       "      <td>leepawg</td>\n",
       "      <td>2021-01-29 16:54:43</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>At This Point It’s 🧻🤲🏽 vs 💎👐💎</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>0.98</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2021-01-29</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44750</th>\n",
       "      <td>n0o8wq</td>\n",
       "      <td>5EQiByvHDu</td>\n",
       "      <td>2021-04-28 20:39:18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Facebook's Q1 2021 Earnings: 48% Revenue Growt...</td>\n",
       "      <td>none</td>\n",
       "      <td>0.95</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>2021-04-28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id          author              created  removed  deleted  is_self  \\\n",
       "12125  rssid0     NoMoneyNoMe  2021-12-31 12:04:49        0        0        1   \n",
       "40609  lcbahe  Suicidal_Donut  2021-02-04 08:30:59        0        0        1   \n",
       "10783  rpulu4      _STIFFL3R_  2021-12-27 19:03:39        0        0        0   \n",
       "19135  l7xjio         leepawg  2021-01-29 16:54:43        1        0        1   \n",
       "44750  n0o8wq      5EQiByvHDu  2021-04-28 20:39:18        0        0        1   \n",
       "\n",
       "       is_video                                              title  \\\n",
       "12125         0  How to profit from the upcoming war between NA...   \n",
       "40609         0                  What do you think people will do?   \n",
       "10783         0                   Marry xmas and a happy new year!   \n",
       "19135         0                      At This Point It’s 🧻🤲🏽 vs 💎👐💎   \n",
       "44750         0  Facebook's Q1 2021 Earnings: 48% Revenue Growt...   \n",
       "\n",
       "      link_flair_text  upvote_ratio  ...  word_count_title        date  \\\n",
       "12125      Discussion          0.52  ...                11  2021-12-31   \n",
       "40609      Discussion          0.82  ...                 7  2021-02-04   \n",
       "10783            Loss          0.96  ...                 7  2021-12-27   \n",
       "19135      Discussion          0.98  ...                 7  2021-01-29   \n",
       "44750            none          0.95  ...                 9  2021-04-28   \n",
       "\n",
       "      neg_ttl_v neu_ttl_v pos_ttl_v  compound_ttl_v  neg_st_v neu_st_v  \\\n",
       "12125      0.25      0.57      0.18           -0.25      0.18     0.77   \n",
       "40609      0.00      1.00      0.00            0.00      0.06     0.91   \n",
       "10783      0.00      0.60      0.40            0.61      0.00     1.00   \n",
       "19135      0.00      1.00      0.00            0.00      0.00     1.00   \n",
       "44750      0.00      0.76      0.24            0.38      0.01     0.83   \n",
       "\n",
       "       pos_st_v  compound_st_v  \n",
       "12125      0.05          -0.93  \n",
       "40609      0.03          -0.51  \n",
       "10783      0.00           0.00  \n",
       "19135      0.00           0.00  \n",
       "44750      0.16           0.97  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Check the distribution of average VADER sentiment scores over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4x2 grid of subplots\n",
    "fig, axes = plt.subplots(nrows=4, ncols=2, figsize=(10, 10))\n",
    "sentiment_titles = ['Negative Sentiment', 'Neutral Sentiment', 'Positive Sentiment', 'Compound Sentiment']\n",
    "\n",
    "# The column pairs for sentiment values\n",
    "sentiment_pairs = [('neg_st_v', 'neg_ttl_v'), \n",
    "                   ('neu_st_v', 'neu_ttl_v'), \n",
    "                   ('pos_st_v', 'pos_ttl_v'), \n",
    "                   ('compound_st_v', 'compound_ttl_v')]\n",
    "\n",
    "for i, (sentiment_selftext, sentiment_title) in enumerate(sentiment_pairs):\n",
    "    # Plot for 'selftext'\n",
    "    axes[i, 0].hist(df[sentiment_selftext].dropna(), bins=50, color='skyblue', alpha=0.7)\n",
    "    axes[i, 0].set_title(f'Selftext {sentiment_titles[i]}')\n",
    "    axes[i, 0].set_xlabel('Score')\n",
    "    axes[i, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Plot for 'title'\n",
    "    axes[i, 1].hist(df[sentiment_title].dropna(), bins=50, color='salmon', alpha=0.7)\n",
    "    axes[i, 1].set_title(f'Title {sentiment_titles[i]}')\n",
    "    axes[i, 1].set_xlabel('Score')\n",
    "    # The ylabel only needs to be set on the leftmost plots\n",
    "    if i == 0:\n",
    "        axes[i, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by the 'Date' column and calculate the average of the 'compound' column\n",
    "df_compound_by_date = df.groupby('date')['compound'].mean()\n",
    "df_by_date = df.groupby('date')['compound'].mean()\n",
    "\n",
    "# Display the result\n",
    "df_compound_by_date\n",
    "df_compound_by_date.plot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Plotting trends in usage of emojis over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where SELFTEXT contains the specified phrase/word/emoji\n",
    "filtered_df = df[df['selftext'].str.contains(\"💎\", na=False, regex=True)]\n",
    "filtered_df\n",
    "\n",
    "# Group by the 'Date' column and sum specific numerical columns\n",
    "df_words_by_date = filtered_df.groupby('date')['id'].count()\n",
    "\n",
    "# Display the result\n",
    "df_words_by_date.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for rows where TITLE contains the specified phrase\n",
    "filtered_df = df[df['title'].str.contains(\"💎\", na=False, regex=True)]\n",
    "filtered_df\n",
    "\n",
    "# Group by the 'Date' column and sum specific numerical columns\n",
    "df_words_by_date = filtered_df.groupby('date')['id'].count()\n",
    "df_by_date = df.groupby('date')['id'].count()\n",
    "df_words_by_date_pct = df_words_by_date / df_by_date * 100\n",
    "\n",
    "# Display the result in PERCENTAGES\n",
    "df_words_by_date_pct.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of words with bullish sentiment\n",
    "bull_list = ['apes', 'DFV', 'hodling', 'HODL', 'diamond', 'gme', 'GME', 'moon', 'rocket']\n",
    "\n",
    "# Escape each word in the bull list to ensure special regex characters are treated literally\n",
    "escaped_bull_list = [re.escape(word) for word in bull_list]\n",
    "\n",
    "# Create a regular expression pattern to match any of the words in the escaped bull list\n",
    "pattern = r'\\b(?:' + '|'.join(escaped_bull_list) + r')\\b'\n",
    "\n",
    "# Go through 'selftext' and 'title' for the presence of any of the words in the pattern\n",
    "df.loc[:, 'bullflag'] = df['selftext'].str.contains(pattern, na=False, case=False) | \\\n",
    "                        df['title'].str.contains(pattern, na=False, case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of emojis with bullish sentiment\n",
    "emoji_list = ['💎', '🦍', '🚀', '🙌']\n",
    "\n",
    "# A function to create a valid column name for an emoji\n",
    "def emoji_to_column_name(emoji):\n",
    "    # Use a description or a numeric code that represents the emoji\n",
    "    emoji_names = {\n",
    "        '💎': 'e_diamond',\n",
    "        '🦍': 'e_gorilla',\n",
    "        '🚀': 'e_rocket',\n",
    "        '🙌': 'e_hands'\n",
    "    }\n",
    "    return 'flag_' + emoji_names.get(emoji, 'emoji')\n",
    "\n",
    "# Iterate over the list of emojis\n",
    "for emoji in emoji_list:\n",
    "    # Create a regular expression pattern for the current emoji\n",
    "    pattern = re.escape(emoji)\n",
    "    \n",
    "    # Create a new column for each emoji in emoji_list\n",
    "    column_name = emoji_to_column_name(emoji)\n",
    "    df.loc[:, column_name] = df['selftext'].str.contains(pattern, na=False) | \\\n",
    "                             df['title'].str.contains(pattern, na=False)\n",
    "    \n",
    "    # Convert the boolean flags to integers to create dummy variables\n",
    "    df.loc[:, column_name + '_dummy'] = df[column_name].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns to drop, all that don't end with '_dummy'\n",
    "columns_to_drop = [col for col in df.columns if col.startswith('flag_') and not col.endswith('_dummy')]\n",
    "\n",
    "# Drop the selected columns from the DataFrame\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'date' and count the flagged rows.\n",
    "date_grouped = df[df['bullflag']].groupby('date').size()\n",
    "\n",
    "# Now we will get the total counts per 'date' in the original dataframe for normalization.\n",
    "total_counts_by_date = df.groupby('date').size()\n",
    "\n",
    "# Calculate the percentages.\n",
    "percentages = (date_grouped / total_counts_by_date) * 100\n",
    "\n",
    "# Plot the result in percentages.\n",
    "percentages.plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15, 5), sharey=True)\n",
    "\n",
    "# Define a function to filter, group, and plot the data based on the deletion status.\n",
    "def plot_flagged_percentage_subplot(df, delete_status, ax):\n",
    "    # Filter the DataFrame for the given delete_status and where 'flag' is True.\n",
    "    flagged = df[(df['removed'] == delete_status) & df['bullflag']]\n",
    "    \n",
    "    # Group by 'date' and count the occurrences.\n",
    "    flagged_by_date = flagged.groupby('date').size()\n",
    "    \n",
    "    # Calculate the percentages.\n",
    "    percentages = (flagged_by_date / total_counts_by_date) * 100\n",
    "    \n",
    "    # Plot the percentages on the provided subplot axis.\n",
    "    percentages.plot.line(ax=ax)\n",
    "    ax.set_title(f\"'deleted' = {delete_status}\")\n",
    "    ax.set_ylabel('Percentage')\n",
    "    ax.set_xlabel('Date')\n",
    "\n",
    "# Call the function for both deletion statuses with the respective subplot axis.\n",
    "plot_flagged_percentage_subplot(df, 1, axes[0])  # For 'deleted' = 1\n",
    "plot_flagged_percentage_subplot(df, 0, axes[1])  # For 'deleted' = 0\n",
    "\n",
    "# Improve spacing between plots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/reddit_cleaned_vader_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using regex for extracting ticker from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified code (need source forgot where it came from )\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Function to find all occurrences of the stock ticker in the text blob\n",
    "def find_occurrences_of_stock_ticker(arg_ticker, arg_text_to_search):\n",
    "    # Case-insensitive search for the ticker and ticker prefixed with a '$'\n",
    "    reg_ex_count = sum(1 for match in re.finditer(r'\\b({}|${})\\b'.format(arg_ticker, arg_ticker), \n",
    "                                                  arg_text_to_search, re.IGNORECASE))\n",
    "    return reg_ex_count\n",
    "\n",
    "# Load your tickers list from the CSV file\n",
    "tickers_df = pd.read_csv('../data/tickers_list_short.csv')\n",
    "\n",
    "# Assume df is your dataframe with a column 'selftext' containing the comments/posts\n",
    "# Convert the 'selftext' to string to ensure that there are no floats\n",
    "df['selftext'] = df['selftext'].astype(str)\n",
    "\n",
    "# Create a text blob from all the comments/posts\n",
    "text_blob = ' '.join(df['selftext'])\n",
    "\n",
    "# Create a dictionary to hold the count of occurrences for each ticker\n",
    "dictionary = {}\n",
    "\n",
    "# Go through each ticker and count its occurrences in the text blob\n",
    "for index, row in tickers_df.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    occurrences = find_occurrences_of_stock_ticker(ticker, text_blob)\n",
    "    if occurrences > 0:\n",
    "        dictionary[ticker] = occurrences\n",
    "\n",
    "# Now you can sort the dictionary by occurrences and do further analysis\n",
    "# For example, you can convert it to a DataFrame\n",
    "ticker_counts_df = pd.DataFrame(list(dictionary.items()), columns=['Ticker', 'Occurrences']).sort_values(by='Occurrences', ascending=False)\n",
    "\n",
    "# If you want to add this as a column to your original df, we can map the counts back to each comment\n",
    "# We will create a function that maps the occurrences to each comment\n",
    "\n",
    "def map_ticker_occurrences(comment, ticker_counts):\n",
    "    tickers_in_comment = []\n",
    "    for ticker, count in ticker_counts.items():\n",
    "        if ticker in comment.upper():  # Converting to upper case for case-insensitive comparison\n",
    "            tickers_in_comment.append(ticker)\n",
    "    return tickers_in_comment\n",
    "\n",
    "# Map the ticker occurrences back to the original DataFrame\n",
    "df['tickers_mentioned'] = df['selftext'].apply(lambda comment: map_ticker_occurrences(comment, dictionary))\n",
    "\n",
    "# Display the DataFrame with ticker counts\n",
    "print(ticker_counts_df)\n",
    "\n",
    "# Display the DataFrame with tickers mentioned in each comment\n",
    "print(df[['selftext', 'tickers_mentioned']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the ticker occurrences back to the original DataFrame\n",
    "df['tickers_mentioned'] = df['selftext'].apply(lambda comment: map_ticker_occurrences(comment, dictionary))\n",
    "\n",
    "# Display the DataFrame with ticker counts\n",
    "print(ticker_counts_df)\n",
    "\n",
    "# Display the DataFrame with tickers mentioned in each comment\n",
    "print(df[['selftext', 'tickers_mentioned']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Ticker list selection\n",
    "this code was adapted from here: https://github.com/theriley106/TheWSBIndex/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original code written from pseudocode\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Read the stock tickers and company names from a CSV file into a DataFrame\n",
    "tickers_df = pd.read_csv('../data/tickers_list_short.csv')\n",
    "\n",
    "# Create a dictionary to map both tickers and company names to their official ticker symbols\n",
    "# Mapping is done by converting everything to uppercase to ensure case-insensitive matching\n",
    "# It also handles NaN values by checking if the entries are not null\n",
    "ticker_name_map = {\n",
    "    str(row['Ticker']).upper(): row['Ticker'] \n",
    "    for _, row in tickers_df.iterrows() if pd.notnull(row['Ticker'])\n",
    "}\n",
    "ticker_name_map.update({\n",
    "    str(row['Name']).upper(): row['Ticker'] \n",
    "    for _, row in tickers_df.iterrows() if pd.notnull(row['Name'])\n",
    "})\n",
    "\n",
    "# Define the process_comment function that will take a comment string and return a dictionary\n",
    "# containing categorized tickers based on the action words found in the comment\n",
    "def process_comment(comment):\n",
    "    # If comment is not a string (e.g., NaN), return an empty dictionary\n",
    "    if not isinstance(comment, str):\n",
    "        return {'puts': [], 'calls': [], 'buy': [], 'sell': []}\n",
    "\n",
    "    # Initialize a dictionary to store categorized tickers\n",
    "    comment_info = {'puts': [], 'calls': [], 'buy': [], 'sell': []}\n",
    "\n",
    "    # Convert the entire comment to uppercase to match against the ticker_name_map\n",
    "    comment_upper = comment.upper()\n",
    "    \n",
    "    # Use regex to find all words in the comment\n",
    "    words = re.findall(r'\\b\\w+\\b', comment_upper)\n",
    "\n",
    "    # Initialize an empty list to temporarily store tickers as we find them\n",
    "    tempList = []\n",
    "\n",
    "    # Iterate through each word in the comment\n",
    "    for word in words:\n",
    "        # Check if the word is a ticker or company name, adjusting for $ prefix and matching against the map\n",
    "        if word in ticker_name_map or word.strip('$') in ticker_name_map:\n",
    "            # If it is, add the official ticker symbol to tempList\n",
    "            ticker = ticker_name_map.get(word, ticker_name_map.get(word.strip('$'), ''))\n",
    "            tempList.append(ticker)\n",
    "        # Check for buying action words and update the comment_info dictionary\n",
    "        elif word in ['BUY', 'BUYING']:\n",
    "            comment_info['buy'].extend(tempList)\n",
    "            tempList.clear()\n",
    "        # Check for selling action words and update the comment_info dictionary\n",
    "        elif word in ['SELL', 'SOLD', 'CLOSE', 'CLOSING', 'SHORTS']:\n",
    "            comment_info['sell'].extend(tempList)\n",
    "            tempList.clear()\n",
    "        # Check for 'puts' and 'calls' action words and update the comment_info dictionary\n",
    "        elif word == 'PUTS' and tempList:\n",
    "            comment_info['puts'].extend(tempList)\n",
    "            tempList.clear()\n",
    "        elif word == 'CALLS' and tempList:\n",
    "            comment_info['calls'].extend(tempList)\n",
    "            tempList.clear()\n",
    "\n",
    "    # Handle any remaining tickers that haven't been categorized\n",
    "    if tempList:\n",
    "        # By default, we add any uncategorized tickers to the 'buy' category\n",
    "        # Modify as needed based on your logic or requirements\n",
    "        comment_info['buy'].extend(tempList)\n",
    "\n",
    "    # Return the populated dictionary for this comment\n",
    "    return comment_info\n",
    "\n",
    "# Fill NaN values with an empty string before applying the process_comment function\n",
    "df['selftext'] = df['selftext'].fillna('')\n",
    "df['comment_info'] = df['selftext'].apply(process_comment)\n",
    "\n",
    "# Apply the process_comment function only to non-null rows in the 'selftext' column\n",
    "# If 'selftext' is NaN, the corresponding 'comment_info' will be an empty dictionary\n",
    "df['comment_info'] = df['selftext'].dropna().apply(process_comment)\n",
    "\n",
    "# Optionally, if you want to create separate columns for each action\n",
    "# Expand the dictionaries in 'comment_info' into separate columns\n",
    "df[['puts', 'calls', 'buy', 'sell']] = df['comment_info'].apply(pd.Series)\n",
    "\n",
    "# Display the DataFrame to verify the output\n",
    "print(df[['selftext', 'puts', 'calls', 'buy', 'sell']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sell'].value_counts(normalize=True).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to extract stock ticker mentions from the social media posts \n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Sample text containing stock mentions\n",
    "text = \"I just bought $GME and $AAPL, and I'm considering $AMZN as well. $gme is on fire! GME, Gamestop, gamestop, GameStop, gme.\"\n",
    "\n",
    "# Sample list of publicly traded tickers\n",
    "ticker_data = {\n",
    "    'Ticker': ['$GME', '$AAPL', '$AMZN', '$TSLA'],\n",
    "    'Company': ['GameStop Inc.', 'Apple Inc.', 'Amazon.com Inc.', 'Tesla, Inc.']\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the ticker data\n",
    "ticker_df = pd.DataFrame(ticker_data)\n",
    "\n",
    "# Define a regular expression pattern to find stock mentions\n",
    "pattern = r'\\$?(?i)\\b(' + '|'.join(map(re.escape, ticker_df['Ticker'])) + r')\\b'\n",
    "\n",
    "# Find all stock mentions in the text\n",
    "mentions = re.findall(pattern, text)\n",
    "\n",
    "# Remove duplicate mentions\n",
    "unique_mentions = list(set(mentions))\n",
    "\n",
    "# Create a dictionary to store the mentions and associated companies\n",
    "mentions_dict = {}\n",
    "for mention in unique_mentions:\n",
    "    company = ticker_df[ticker_df['Ticker'].str.lower() == mention.lstrip('$').lower()]['Company'].values[0]\n",
    "    mentions_dict[mention] = company\n",
    "\n",
    "# Create a DataFrame from the mentions dictionary\n",
    "mentions_df = pd.DataFrame.from_dict(mentions_dict, orient='index', columns=['Company'])\n",
    "\n",
    "# Print the extracted mentions\n",
    "print(mentions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Define a function to get the most common ticker\n",
    "def get_most_common_ticker(tickers_list):\n",
    "    if not tickers_list:  # If the list is empty\n",
    "        return None  # Or you could return a default string like 'No Tickers'\n",
    "    # Count the tickers and get the most common one\n",
    "    most_common_ticker = Counter(tickers_list).most_common(1)[0][0]\n",
    "    return most_common_ticker\n",
    "\n",
    "# Apply this function to each row in the 'buy' column to create a new 'signal' column\n",
    "df['buy_signal'] = df['buy'].apply(get_most_common_ticker)\n",
    "df['buy_signal'].value_counts(normalize=True)\n",
    "\n",
    "# Apply this function to each row in the 'buy' column to create a new 'signal' column\n",
    "df['sell_signal'] = df['sell'].apply(get_most_common_ticker)\n",
    "df['sell_signal'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script connects to reddit's API and retrieves a count of stock $ticker mentions to gauge rising popularity.\n",
    "# from https://github.com/DrakeDavis/RedditApiStockParser/blob/main/parse_tickers_from_reddit.py\n",
    "import praw\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "from datetime import timezone\n",
    "import re\n",
    "import boto3\n",
    "import os\n",
    "import pytz\n",
    "import sys\n",
    "\n",
    "# Function to find all occurrences of the stock ticker (or $ticker) with case ignored. Returns the number of occurrences\n",
    "def find_occurrences_of_stock_ticker(arg_ticker, arg_text_to_search):\n",
    "    # Regex that also checks for boundaries (start of sentence, end of sentence, etc.)\n",
    "    reg_ex_count = sum(1 for match in re.finditer(r\"\\b{}\\b\".format(arg_ticker), arg_text_to_search, re.IGNORECASE))\n",
    "\n",
    "    # Also check for ticker with a $ in front of it\n",
    "    prefaced_ticker = \"$\" + arg_ticker\n",
    "    reg_ex_count = reg_ex_count + sum(1 for match in re.finditer(r\"\\b{}\\b\".format(prefaced_ticker), arg_text_to_search,\n",
    "                                                                 re.IGNORECASE))\n",
    "    return reg_ex_count\n",
    "\n",
    "\n",
    "# Connection credentials to reddit's API\n",
    "reddit = praw.Reddit(\n",
    "    client_id=os.environ['REDDIT_API_CLIENT_ID'],\n",
    "    client_secret=os.environ['REDDIT_API_CLIENT_SECRET'],\n",
    "    user_agent=os.environ['REDDIT_API_USER_AGENT']\n",
    ")\n",
    "\n",
    "# Instantiating objects\n",
    "posts_in_last_day = []\n",
    "text_blob = ''\n",
    "\n",
    "# Retrieve subreddit name from terminal argument\n",
    "subreddit_name = str(sys.argv[1])\n",
    "\n",
    "# Get all posts from subreddit in the last 24 hours (limit is 900, but no 24 period has reached that number)\n",
    "for post in reddit.subreddit(subreddit_name).new(limit=900):\n",
    "    post_title = post.title\n",
    "    post_creation_epoch_time = post.created - 60 * 60 * 8  # subtracting 8 hours due to timezone\n",
    "    current_epoch_time = int(time.time())\n",
    "    age_of_post_in_hours = (current_epoch_time - post_creation_epoch_time) / 60 / 60\n",
    "\n",
    "    if age_of_post_in_hours < 24:\n",
    "        posts_in_last_day.append(post)\n",
    "\n",
    "# Define metrics for posts and comments in the last 24 hours\n",
    "post_count_in_last_day = posts_in_last_day.__len__()\n",
    "comments_in_last_day = 0\n",
    "\n",
    "# Retrieve all comments from the acquired posts\n",
    "for post in posts_in_last_day:\n",
    "    text_blob = text_blob + post.title\n",
    "    post.comments.replace_more(limit=1)\n",
    "    for comment in post.comments.list():\n",
    "        if comment.body:\n",
    "            comments_in_last_day = comments_in_last_day + 1\n",
    "            text_blob = text_blob + comment.body\n",
    "\n",
    "# The text_blob is an amalgamation of all posts and comments from the last 24 hours\n",
    "# We're going to parse it and find occurrences of stock names\n",
    "dictionary = {}\n",
    "with open(\"curated_stock_tickers.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\n')\n",
    "        print(\"Currently counting: \" + str(line))\n",
    "        occurrences = find_occurrences_of_stock_ticker(line, text_blob)\n",
    "        if occurrences > 0:\n",
    "            dictionary[line] = occurrences\n",
    "\n",
    "# Get the current time and format it accordingly\n",
    "current_time = datetime.datetime.now(timezone.utc)\n",
    "est = pytz.timezone('US/Eastern')\n",
    "date_format = \"%d %B %I:%M %p\"\n",
    "\n",
    "# Write out the data in .json format for consumption by the frontend\n",
    "json_data = {\"posts\": post_count_in_last_day, \"comments\": comments_in_last_day,\n",
    "             \"time\": current_time.astimezone(est).strftime(date_format),\n",
    "             \"data\": (sorted(dictionary.items(), key=lambda x: x[1], reverse=True))}\n",
    "fp = open(subreddit_name + '_most_mentioned_stocks.json', 'w+')\n",
    "fp.write(json.dumps(json_data))\n",
    "fp.close()\n",
    "\n",
    "# Open connection to AWS S3 bucket\n",
    "s3 = boto3.resource('s3',\n",
    "                    aws_access_key_id=os.environ['S3_KEY'],\n",
    "                    aws_secret_access_key=os.environ['S3_SECRET'])\n",
    "s3_client = boto3.client('s3',\n",
    "                         aws_access_key_id=os.environ['S3_KEY'],\n",
    "                         aws_secret_access_key=os.environ['S3_SECRET'])\n",
    "\n",
    "# Upload the .json file to S3. Making it public so anyone can use it.\n",
    "s3_client.upload_file(subreddit_name + '_most_mentioned_stocks.json', 'wsb-pop-index',\n",
    "                        subreddit_name + 'PopIndex.json', ExtraArgs={'ContentType': \"application/json\",\n",
    "                        'ACL': 'public-read'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['selftext', ].values[105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Options: Spark NLP? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.x. Vectorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CountVectorizer and HashingTF from PySpark to transform textual data into vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.x. Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.x. Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k-means++ clustering from both PySpark's machine learning library and scikit-learn, combined with PCA and TruncatedSVD/t-SNE from scikit-learn for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
