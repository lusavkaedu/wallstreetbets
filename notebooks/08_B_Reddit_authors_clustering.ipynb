{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 4. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAFILE = '../data/04_Reddit_model_applied_results.csv'\n",
    "df_all = pd.read_csv(DATAFILE, index_col = 'id')\n",
    "df = df_all.sample(30000).copy()\n",
    "# df = df_all.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, l72klk to l8kggs\n",
      "Data columns (total 22 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   author               30000 non-null  object \n",
      " 1   created              30000 non-null  object \n",
      " 2   removed              30000 non-null  float64\n",
      " 3   deleted              30000 non-null  float64\n",
      " 4   is_self              30000 non-null  float64\n",
      " 5   is_video             30000 non-null  float64\n",
      " 6   link_flair_text      30000 non-null  object \n",
      " 7   upvote_ratio         30000 non-null  float64\n",
      " 8   score                30000 non-null  float64\n",
      " 9   num_comments         30000 non-null  float64\n",
      " 10  shortlink            30000 non-null  object \n",
      " 11  FolderName           30000 non-null  object \n",
      " 12  word_count_selftext  30000 non-null  float64\n",
      " 13  word_count_title     30000 non-null  float64\n",
      " 14  date                 30000 non-null  object \n",
      " 15  Text                 30000 non-null  object \n",
      " 16  Probab_not_rel       30000 non-null  float64\n",
      " 17  Probab_yes_rel       30000 non-null  float64\n",
      " 18  Post Relevance       30000 non-null  int64  \n",
      " 19  Prob SELL            30000 non-null  float64\n",
      " 20  Prob BUY             30000 non-null  float64\n",
      " 21  BUY SELL categories  30000 non-null  int64  \n",
      "dtypes: float64(13), int64(2), object(7)\n",
      "memory usage: 5.3+ MB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "df.columns\n",
    "df.info()\n",
    "# Filling in empty cells for selftext field\n",
    "df['Text'].fillna('notextprovided', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df = df.drop(['created', 'shortlink','date'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_flair_text\n",
       "Discussion     7773\n",
       "none           4322\n",
       "YOLO           3364\n",
       "Meme           3196\n",
       "Gain           1781\n",
       "               ... \n",
       "üêçDebunkedüêç        1\n",
       "Baby üêã            1\n",
       "Research/DD       1\n",
       "Rule 2            1\n",
       "HODL üíéüôå           1\n",
       "Name: count, Length: 147, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting unique values for 'FolderName' and 'link_flair_text'\n",
    "folder_name_unique_counts = df['FolderName'].value_counts()\n",
    "link_flair_text_unique_counts = df['link_flair_text'].value_counts()\n",
    "\n",
    "link_flair_text_unique_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing one-hot encoding on 'FolderName' and 'link_flair_text'\n",
    "#df = pd.get_dummies(df, columns=['FolderName'], drop_first=True)\n",
    "#df = pd.get_dummies(df, columns=['link_flair_text'], drop_first=True)\n",
    "\n",
    "# Converting the one-hot encoded columns to integers\n",
    "#one_hot_columns = df.columns[df.columns.str.startswith('FolderName_') | df.columns.str.startswith('link_flair_text_')]\n",
    "#df[one_hot_columns] = df[one_hot_columns].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>removed</th>\n",
       "      <th>deleted</th>\n",
       "      <th>is_self</th>\n",
       "      <th>is_video</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>word_count_selftext</th>\n",
       "      <th>word_count_title</th>\n",
       "      <th>Probab_not_rel</th>\n",
       "      <th>Probab_yes_rel</th>\n",
       "      <th>Post Relevance</th>\n",
       "      <th>Prob SELL</th>\n",
       "      <th>Prob BUY</th>\n",
       "      <th>BUY SELL categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>30000.000000</td>\n",
       "      <td>3.000000e+04</td>\n",
       "      <td>30000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.704767</td>\n",
       "      <td>0.092300</td>\n",
       "      <td>0.603833</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.902837</td>\n",
       "      <td>68.746333</td>\n",
       "      <td>8.935133</td>\n",
       "      <td>27.166300</td>\n",
       "      <td>9.850867</td>\n",
       "      <td>0.699577</td>\n",
       "      <td>0.300423</td>\n",
       "      <td>0.209967</td>\n",
       "      <td>0.350296</td>\n",
       "      <td>6.497040e-01</td>\n",
       "      <td>0.712567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.456155</td>\n",
       "      <td>0.289454</td>\n",
       "      <td>0.489108</td>\n",
       "      <td>0.153672</td>\n",
       "      <td>0.173906</td>\n",
       "      <td>971.598326</td>\n",
       "      <td>49.305845</td>\n",
       "      <td>133.292761</td>\n",
       "      <td>8.726872</td>\n",
       "      <td>0.253987</td>\n",
       "      <td>0.253987</td>\n",
       "      <td>0.407291</td>\n",
       "      <td>0.290903</td>\n",
       "      <td>2.909031e-01</td>\n",
       "      <td>0.452573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000349</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3.736543e-07</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.870000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.562515</td>\n",
       "      <td>0.101295</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.095518</td>\n",
       "      <td>4.459711e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.792342</td>\n",
       "      <td>0.207658</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.280088</td>\n",
       "      <td>7.199120e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>0.898705</td>\n",
       "      <td>0.437485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.554029</td>\n",
       "      <td>9.044821e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>64548.000000</td>\n",
       "      <td>2388.000000</td>\n",
       "      <td>5209.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>0.997128</td>\n",
       "      <td>0.999651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.999911e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            removed       deleted       is_self      is_video  upvote_ratio  \\\n",
       "count  30000.000000  30000.000000  30000.000000  30000.000000  30000.000000   \n",
       "mean       0.704767      0.092300      0.603833      0.024200      0.902837   \n",
       "std        0.456155      0.289454      0.489108      0.153672      0.173906   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.020000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.870000   \n",
       "50%        1.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "75%        1.000000      0.000000      1.000000      0.000000      1.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "              score  num_comments  word_count_selftext  word_count_title  \\\n",
       "count  30000.000000  30000.000000         30000.000000      30000.000000   \n",
       "mean      68.746333      8.935133            27.166300          9.850867   \n",
       "std      971.598326     49.305845           133.292761          8.726872   \n",
       "min        0.000000      0.000000             1.000000          1.000000   \n",
       "25%        1.000000      0.000000             1.000000          4.000000   \n",
       "50%        1.000000      1.000000             1.000000          7.000000   \n",
       "75%        8.000000      4.000000             1.000000         12.000000   \n",
       "max    64548.000000   2388.000000          5209.000000         74.000000   \n",
       "\n",
       "       Probab_not_rel  Probab_yes_rel  Post Relevance     Prob SELL  \\\n",
       "count    30000.000000    30000.000000    30000.000000  30000.000000   \n",
       "mean         0.699577        0.300423        0.209967      0.350296   \n",
       "std          0.253987        0.253987        0.407291      0.290903   \n",
       "min          0.000349        0.002872        0.000000      0.000009   \n",
       "25%          0.562515        0.101295        0.000000      0.095518   \n",
       "50%          0.792342        0.207658        0.000000      0.280088   \n",
       "75%          0.898705        0.437485        0.000000      0.554029   \n",
       "max          0.997128        0.999651        1.000000      1.000000   \n",
       "\n",
       "           Prob BUY  BUY SELL categories  \n",
       "count  3.000000e+04         30000.000000  \n",
       "mean   6.497040e-01             0.712567  \n",
       "std    2.909031e-01             0.452573  \n",
       "min    3.736543e-07             0.000000  \n",
       "25%    4.459711e-01             0.000000  \n",
       "50%    7.199120e-01             1.000000  \n",
       "75%    9.044821e-01             1.000000  \n",
       "max    9.999911e-01             1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author                 0.0\n",
       "removed                0.0\n",
       "deleted                0.0\n",
       "is_self                0.0\n",
       "is_video               0.0\n",
       "link_flair_text        0.0\n",
       "upvote_ratio           0.0\n",
       "score                  0.0\n",
       "num_comments           0.0\n",
       "FolderName             0.0\n",
       "word_count_selftext    0.0\n",
       "word_count_title       0.0\n",
       "Text                   0.0\n",
       "Probab_not_rel         0.0\n",
       "Probab_yes_rel         0.0\n",
       "Post Relevance         0.0\n",
       "Prob SELL              0.0\n",
       "Prob BUY               0.0\n",
       "BUY SELL categories    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(df['score'], bins = 50)\n",
    "plt.axvline(df['score'].mean(), color = 'r')\n",
    "plt.yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering Using Text Complexity Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/lgfolder/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import libraries for data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries for text analysis\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from textstat import textstat, automated_readability_index\n",
    "\n",
    "# Import spaCy for syntactic complexity analysis\n",
    "import spacy\n",
    "\n",
    "# Import tqdm for progress bar (if needed for loops)\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Download necessary NLTK models (if not previously downloaded)\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNDL. My failed short term investment. [removed]\n"
     ]
    }
   ],
   "source": [
    "# Sample one random row and select only the 'Text' field\n",
    "sample_text = df['Text'].sample(1).iloc[0]\n",
    "\n",
    "# Print the sampled text\n",
    "print(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boolean column for authors that messaged about GME\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to check for named entities 'GME', 'gme', 'GameStop', '$gme', '$GME'\n",
    "def contains_gme_or_variants(text):\n",
    "    doc = nlp(text)\n",
    "    target_entities = {'GME', 'gme', 'GameStop', '$gme', '$GME'}\n",
    "    return any(ent.text in target_entities for ent in doc.ents)\n",
    "\n",
    "# Apply the function to each row in the 'Text' column and create a new boolean column\n",
    "df['Contains_GME_Variants'] = df['Text'].apply(contains_gme_or_variants).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a boolean column for authors that messaged about AMC\n",
    "# Function to check for named entities 'AMC', 'amc','$amc', '$AMC'\n",
    "def contains_amc_or_variants(text):\n",
    "    doc = nlp(text)\n",
    "    target_entities = {'AMC', 'amc','$amc', '$AMC'}\n",
    "    return any(ent.text in target_entities for ent in doc.ents)\n",
    "\n",
    "# Apply the function to each row in the 'Text' column and create a new boolean column\n",
    "df['Contains_AMC_Variants'] = df['Text'].apply(contains_amc_or_variants).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate readability score\n",
    "def calc_readability(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df['Readability_Score'] = df['Text'].apply(calc_readability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate vocabulary complexity\n",
    "def calc_vocab_complexity(text):\n",
    "    words = word_tokenize(text)\n",
    "    freq_dist = FreqDist(words)\n",
    "    return len(freq_dist) / float(len(words)) if len(words) > 0 else 0\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df['Vocab_Complexity'] = df['Text'].apply(calc_vocab_complexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count sentences and calculate average sentence length\n",
    "def analyze_text(text):\n",
    "    doc = nlp(text)\n",
    "    num_sentences = len(list(doc.sents))\n",
    "    total_words = sum(len(sentence) for sentence in doc.sents)\n",
    "    avg_sentence_length = total_words / num_sentences if num_sentences > 0 else 0\n",
    "    return num_sentences, avg_sentence_length\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df[['Num_Sentences', 'Avg_Sentence_Length']] = df['Text'].apply(lambda x: pd.Series(analyze_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate Average Readability (ARI) score\n",
    "def calc_ari(text):\n",
    "    return automated_readability_index(text)\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df['ARI_Score'] = df['Text'].apply(calc_ari)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, l72klk to l8kggs\n",
      "Data columns (total 26 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   author                 30000 non-null  object \n",
      " 1   removed                30000 non-null  float64\n",
      " 2   deleted                30000 non-null  float64\n",
      " 3   is_self                30000 non-null  float64\n",
      " 4   is_video               30000 non-null  float64\n",
      " 5   link_flair_text        30000 non-null  object \n",
      " 6   upvote_ratio           30000 non-null  float64\n",
      " 7   score                  30000 non-null  float64\n",
      " 8   num_comments           30000 non-null  float64\n",
      " 9   FolderName             30000 non-null  object \n",
      " 10  word_count_selftext    30000 non-null  float64\n",
      " 11  word_count_title       30000 non-null  float64\n",
      " 12  Text                   30000 non-null  object \n",
      " 13  Probab_not_rel         30000 non-null  float64\n",
      " 14  Probab_yes_rel         30000 non-null  float64\n",
      " 15  Post Relevance         30000 non-null  int64  \n",
      " 16  Prob SELL              30000 non-null  float64\n",
      " 17  Prob BUY               30000 non-null  float64\n",
      " 18  BUY SELL categories    30000 non-null  int64  \n",
      " 19  Contains_GME_Variants  30000 non-null  int64  \n",
      " 20  Contains_AMC_Variants  30000 non-null  int64  \n",
      " 21  Readability_Score      30000 non-null  float64\n",
      " 22  Vocab_Complexity       30000 non-null  float64\n",
      " 23  Num_Sentences          30000 non-null  float64\n",
      " 24  Avg_Sentence_Length    30000 non-null  float64\n",
      " 25  ARI_Score              30000 non-null  float64\n",
      "dtypes: float64(18), int64(4), object(4)\n",
      "memory usage: 6.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['link_flair_text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns\n",
    "df = df.drop(['Text'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, l72klk to l8kggs\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   author                 30000 non-null  object \n",
      " 1   removed                30000 non-null  float64\n",
      " 2   deleted                30000 non-null  float64\n",
      " 3   is_self                30000 non-null  float64\n",
      " 4   is_video               30000 non-null  float64\n",
      " 5   upvote_ratio           30000 non-null  float64\n",
      " 6   score                  30000 non-null  float64\n",
      " 7   num_comments           30000 non-null  float64\n",
      " 8   FolderName             30000 non-null  object \n",
      " 9   word_count_selftext    30000 non-null  float64\n",
      " 10  word_count_title       30000 non-null  float64\n",
      " 11  Probab_not_rel         30000 non-null  float64\n",
      " 12  Probab_yes_rel         30000 non-null  float64\n",
      " 13  Post Relevance         30000 non-null  int64  \n",
      " 14  Prob SELL              30000 non-null  float64\n",
      " 15  Prob BUY               30000 non-null  float64\n",
      " 16  BUY SELL categories    30000 non-null  int64  \n",
      " 17  Contains_GME_Variants  30000 non-null  int64  \n",
      " 18  Contains_AMC_Variants  30000 non-null  int64  \n",
      " 19  Readability_Score      30000 non-null  float64\n",
      " 20  Vocab_Complexity       30000 non-null  float64\n",
      " 21  Num_Sentences          30000 non-null  float64\n",
      " 22  Avg_Sentence_Length    30000 non-null  float64\n",
      " 23  ARI_Score              30000 non-null  float64\n",
      "dtypes: float64(18), int64(4), object(2)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many features show a strong skew towards 0, and outliers will tend to skew distance based models. I do a log transform to make these distributions more normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert pennystocks to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1490\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49m_cython_operation(\n\u001b[1;32m   1491\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39maggregate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1492\u001b[0m         values,\n\u001b[1;32m   1493\u001b[0m         how,\n\u001b[1;32m   1494\u001b[0m         axis\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m,\n\u001b[1;32m   1495\u001b[0m         min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[1;32m   1496\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:959\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[0;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m ngroups \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngroups\n\u001b[0;32m--> 959\u001b[0m \u001b[39mreturn\u001b[39;00m cy_op\u001b[39m.\u001b[39;49mcython_operation(\n\u001b[1;32m    960\u001b[0m     values\u001b[39m=\u001b[39;49mvalues,\n\u001b[1;32m    961\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    962\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[1;32m    963\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mids,\n\u001b[1;32m    964\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[1;32m    965\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    966\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:657\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[0;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[1;32m    650\u001b[0m         values,\n\u001b[1;32m    651\u001b[0m         min_count\u001b[39m=\u001b[39mmin_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    655\u001b[0m     )\n\u001b[0;32m--> 657\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_op_ndim_compat(\n\u001b[1;32m    658\u001b[0m     values,\n\u001b[1;32m    659\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[1;32m    660\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[1;32m    661\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[1;32m    662\u001b[0m     mask\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    663\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    664\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:497\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39mreturn\u001b[39;00m res\u001b[39m.\u001b[39mT\n\u001b[0;32m--> 497\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_cython_op(\n\u001b[1;32m    498\u001b[0m     values,\n\u001b[1;32m    499\u001b[0m     min_count\u001b[39m=\u001b[39;49mmin_count,\n\u001b[1;32m    500\u001b[0m     ngroups\u001b[39m=\u001b[39;49mngroups,\n\u001b[1;32m    501\u001b[0m     comp_ids\u001b[39m=\u001b[39;49mcomp_ids,\n\u001b[1;32m    502\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m    503\u001b[0m     result_mask\u001b[39m=\u001b[39;49mresult_mask,\n\u001b[1;32m    504\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    505\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:541\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m out_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_output_shape(ngroups, values)\n\u001b[0;32m--> 541\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_cython_function(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkind, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhow, values\u001b[39m.\u001b[39;49mdtype, is_numeric)\n\u001b[1;32m    542\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_cython_vals(values)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:173\u001b[0m, in \u001b[0;36mWrappedCythonOp._get_cython_function\u001b[0;34m(cls, kind, how, dtype, is_numeric)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mobject\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39m__signatures__:\n\u001b[1;32m    172\u001b[0m     \u001b[39m# raise NotImplementedError here rather than TypeError later\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    174\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfunction is not implemented for this dtype: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[how->\u001b[39m\u001b[39m{\u001b[39;00mhow\u001b[39m}\u001b[39;00m\u001b[39m,dtype->\u001b[39m\u001b[39m{\u001b[39;00mdtype_str\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m f\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: function is not implemented for this dtype: [how->mean,dtype->object]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:1692\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1692\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39;49m(x)\n\u001b[1;32m   1693\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1694\u001b[0m     \u001b[39m# e.g. \"1+1j\" or \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'pennystocks'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:1696\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mcomplex\u001b[39;49m(x)\n\u001b[1;32m   1697\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1698\u001b[0m     \u001b[39m# e.g. \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: complex() arg is a malformed string",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/05_B_Reddit_Clustering.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/05_B_Reddit_Clustering.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Grouping by 'author' and calculating the mean for each numeric column\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lgfolder/github/capstone-project-lusavkaedu/notebooks/05_B_Reddit_Clustering.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_agg_by_author \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mgroupby(\u001b[39m'\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mmean()\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1855\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   1854\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1855\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cython_agg_general(\n\u001b[1;32m   1856\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1857\u001b[0m         alt\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[1;32m   1858\u001b[0m         numeric_only\u001b[39m=\u001b[39;49mnumeric_only,\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[1;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1507\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39mndim, alt\u001b[39m=\u001b[39malt)\n\u001b[1;32m   1505\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m new_mgr \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39;49mgrouped_reduce(array_func)\n\u001b[1;32m   1508\u001b[0m res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   1509\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/internals/managers.py:1503\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[39mif\u001b[39;00m blk\u001b[39m.\u001b[39mis_object:\n\u001b[1;32m   1500\u001b[0m     \u001b[39m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[39m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[39mfor\u001b[39;00m sb \u001b[39min\u001b[39;00m blk\u001b[39m.\u001b[39m_split():\n\u001b[0;32m-> 1503\u001b[0m         applied \u001b[39m=\u001b[39m sb\u001b[39m.\u001b[39;49mapply(func)\n\u001b[1;32m   1504\u001b[0m         result_blocks \u001b[39m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1505\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/internals/blocks.py:329\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39m@final\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Block]:\n\u001b[1;32m    325\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[39m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[39m    one\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     result \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalues, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    331\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1503\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1490\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgrouper\u001b[39m.\u001b[39m_cython_operation(\n\u001b[1;32m   1491\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maggregate\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1492\u001b[0m         values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m     \u001b[39m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[39m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[39m# try to python agg\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[39m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m-> 1503\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_agg_py_fallback(values, ndim\u001b[39m=\u001b[39;49mdata\u001b[39m.\u001b[39;49mndim, alt\u001b[39m=\u001b[39;49malt)\n\u001b[1;32m   1505\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1457\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     ser \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39miloc[:, \u001b[39m0\u001b[39m]\n\u001b[1;32m   1454\u001b[0m \u001b[39m# We do not get here with UDFs, so we know that our dtype\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[39m#  should always be preserved by the implemented aggregations\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[39m# TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\u001b[39;00m\n\u001b[0;32m-> 1457\u001b[0m res_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgrouper\u001b[39m.\u001b[39;49magg_series(ser, alt, preserve_dtype\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   1459\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(values, Categorical):\n\u001b[1;32m   1460\u001b[0m     \u001b[39m# Because we only get here with known dtype-preserving\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[39m#  reductions, we cast back to Categorical.\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[39m# TODO: if we ever get \"rank\" working, exclude it here.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     res_values \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(values)\u001b[39m.\u001b[39m_from_sequence(res_values, dtype\u001b[39m=\u001b[39mvalues\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:994\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(obj) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(obj\u001b[39m.\u001b[39m_values, np\u001b[39m.\u001b[39mndarray):\n\u001b[1;32m    988\u001b[0m     \u001b[39m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[39m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[39m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[39m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     preserve_dtype \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    996\u001b[0m npvalues \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/ops.py:1015\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m   1012\u001b[0m splitter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_splitter(obj, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1014\u001b[0m \u001b[39mfor\u001b[39;00m i, group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(splitter):\n\u001b[0;32m-> 1015\u001b[0m     res \u001b[39m=\u001b[39m func(group)\n\u001b[1;32m   1016\u001b[0m     res \u001b[39m=\u001b[39m libreduction\u001b[39m.\u001b[39mextract_result(res)\n\u001b[1;32m   1018\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m initialized:\n\u001b[1;32m   1019\u001b[0m         \u001b[39m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:1857\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   1854\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1855\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   1856\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m-> 1857\u001b[0m         alt\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: Series(x)\u001b[39m.\u001b[39;49mmean(numeric_only\u001b[39m=\u001b[39;49mnumeric_only),\n\u001b[1;32m   1858\u001b[0m         numeric_only\u001b[39m=\u001b[39mnumeric_only,\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[1;32m   1860\u001b[0m     \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgroupby\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/generic.py:11556\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11539\u001b[0m \u001b[39m@doc\u001b[39m(\n\u001b[1;32m  11540\u001b[0m     _num_doc,\n\u001b[1;32m  11541\u001b[0m     desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mReturn the mean of the values over the requested axis.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11554\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m  11555\u001b[0m ):\n\u001b[0;32m> 11556\u001b[0m     \u001b[39mreturn\u001b[39;00m NDFrame\u001b[39m.\u001b[39;49mmean(\u001b[39mself\u001b[39;49m, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/generic.py:11201\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11194\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmean\u001b[39m(\n\u001b[1;32m  11195\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m  11196\u001b[0m     axis: Axis \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11199\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m  11200\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Series \u001b[39m|\u001b[39m \u001b[39mfloat\u001b[39m:\n\u001b[0;32m> 11201\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stat_function(\n\u001b[1;32m  11202\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m, nanops\u001b[39m.\u001b[39;49mnanmean, axis, skipna, numeric_only, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m  11203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/generic.py:11158\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11154\u001b[0m     nv\u001b[39m.\u001b[39mvalidate_stat_func((), kwargs, fname\u001b[39m=\u001b[39mname)\n\u001b[1;32m  11156\u001b[0m validate_bool_kwarg(skipna, \u001b[39m\"\u001b[39m\u001b[39mskipna\u001b[39m\u001b[39m\"\u001b[39m, none_allowed\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m> 11158\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reduce(\n\u001b[1;32m  11159\u001b[0m     func, name\u001b[39m=\u001b[39;49mname, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, numeric_only\u001b[39m=\u001b[39;49mnumeric_only\n\u001b[1;32m  11160\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/series.py:4670\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m   4666\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSeries.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m does not allow \u001b[39m\u001b[39m{\u001b[39;00mkwd_name\u001b[39m}\u001b[39;00m\u001b[39m=\u001b[39m\u001b[39m{\u001b[39;00mnumeric_only\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mwith non-numeric dtypes.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   4668\u001b[0m     )\n\u001b[1;32m   4669\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 4670\u001b[0m     \u001b[39mreturn\u001b[39;00m op(delegate, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(invalid\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[39m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[39m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m is_object_dtype(args[\u001b[39m0\u001b[39m]):\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    156\u001b[0m         result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39maxis, skipna\u001b[39m=\u001b[39mskipna, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    157\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     result \u001b[39m=\u001b[39m alt(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    160\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:421\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[39mif\u001b[39;00m datetimelike \u001b[39mand\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     mask \u001b[39m=\u001b[39m isna(values)\n\u001b[0;32m--> 421\u001b[0m result \u001b[39m=\u001b[39m func(values, axis\u001b[39m=\u001b[39;49maxis, skipna\u001b[39m=\u001b[39;49mskipna, mask\u001b[39m=\u001b[39;49mmask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    423\u001b[0m \u001b[39mif\u001b[39;00m datetimelike:\n\u001b[1;32m    424\u001b[0m     result \u001b[39m=\u001b[39m _wrap_results(result, orig_values\u001b[39m.\u001b[39mdtype, fill_value\u001b[39m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:727\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    724\u001b[0m     dtype_count \u001b[39m=\u001b[39m dtype\n\u001b[1;32m    726\u001b[0m count \u001b[39m=\u001b[39m _get_counts(values\u001b[39m.\u001b[39mshape, mask, axis, dtype\u001b[39m=\u001b[39mdtype_count)\n\u001b[0;32m--> 727\u001b[0m the_sum \u001b[39m=\u001b[39m _ensure_numeric(values\u001b[39m.\u001b[39;49msum(axis, dtype\u001b[39m=\u001b[39;49mdtype_sum))\n\u001b[1;32m    729\u001b[0m \u001b[39mif\u001b[39;00m axis \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(the_sum, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    730\u001b[0m     count \u001b[39m=\u001b[39m cast(np\u001b[39m.\u001b[39mndarray, count)\n",
      "File \u001b[0;32m~/anaconda3/envs/myenv2/lib/python3.8/site-packages/pandas/core/nanops.py:1699\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1696\u001b[0m             x \u001b[39m=\u001b[39m \u001b[39mcomplex\u001b[39m(x)\n\u001b[1;32m   1697\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m   1698\u001b[0m             \u001b[39m# e.g. \"foo\"\u001b[39;00m\n\u001b[0;32m-> 1699\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCould not convert \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m to numeric\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert pennystocks to numeric"
     ]
    }
   ],
   "source": [
    "# Grouping by 'author' and calculating the mean for each numeric column\n",
    "df_agg_by_author = df.groupby('author').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg_by_author.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['author'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 99.5th percentile for 'score' and 'num_comments'\n",
    "score_995_percentile = df_agg_by_author['score'].quantile(0.995)\n",
    "num_comments_995_percentile = df_agg_by_author['num_comments'].quantile(0.995)\n",
    "\n",
    "# Calculate the median values for 'score' and 'num_comments'\n",
    "median_score = df_agg_by_author['score'].median()\n",
    "median_num_comments = df_agg_by_author['num_comments'].median()\n",
    "\n",
    "# Replace values above the 99.5th percentile with the median\n",
    "df_agg_by_author.loc[df_agg_by_author['score'] > score_995_percentile, 'score'] = median_score\n",
    "df_agg_by_author.loc[df_agg_by_author['num_comments'] > num_comments_995_percentile, 'num_comments'] = median_num_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_scaler = StandardScaler()\n",
    "X = my_scaler.fit_transform(df_agg_by_author)\n",
    "df_scaled = pd.DataFrame(X, columns = df_agg_by_author.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 15)\n",
    "pca_data = pca.fit_transform(df_scaled)\n",
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize = (8, 8))\n",
    "#sns.pairplot(pd.DataFrame(pca_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t-SNE is a visualisation technique that ignores all information / variance in the dataset, and only preserves the relative positions of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime = 1m 23s seconds for 50000 sample \n",
    "\n",
    "sample = df_scaled.sample(frac = 0.1, random_state = 1)\n",
    "\n",
    "tsne = TSNE(n_components = 3, random_state = 1, verbose = 1)\n",
    "tsne_data = tsne.fit_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(pd.DataFrame(tsne_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "* Data points that were close together in XX(many) -dimensional space, are still close together in the new 3-dimensional space.\n",
    "* There is one large blob, and one very small blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime is very long - 2m 30 sec for 5000 rows\n",
    "\n",
    "inertias = []\n",
    "silhouettes = []\n",
    "k_range = np.arange(2, 15)\n",
    "\n",
    "for k in k_range:\n",
    "  print(\"In progress: K = \", k)\n",
    "  my_kmeans = KMeans(n_clusters = k)\n",
    "  my_kmeans.fit(df_scaled)\n",
    "  labels = my_kmeans.predict(df_scaled)\n",
    "\n",
    "  inertias.append(my_kmeans.inertia_)\n",
    "  silhouettes.append(silhouette_score(df_scaled, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our inertias in a Scree Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_range, inertias, marker = 'o')\n",
    "plt.xlabel(\"Nr clusters\")\n",
    "plt.ylabel(\"Inertia\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no obvious elbow point, after which adding more clusters has very little effect.\n",
    "Let's use our silhouette scores as a second opinion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(k_range, silhouettes, marker='o')\n",
    "plt.xlabel('Nr clusters')\n",
    "plt.ylabel('Silhouette score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Purely based on silhouette scores, where bigger is better, we would be tempted to try 100 clusters, but let's explore other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans with 4 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime 1m47s for 5000 sample\n",
    "\n",
    "# cluster the data\n",
    "kmeans = KMeans(n_clusters = 2)\n",
    "labels = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# add labels to original dataframe\n",
    "df_kmeans = df_scaled.copy()\n",
    "df_kmeans['labels'] = labels\n",
    "\n",
    "# visualise with t-sne\n",
    "sample = df_kmeans.sample(frac = 0.4, random_state = 1)\n",
    "tsne = TSNE(n_components = 3, verbose=1, random_state = 1)\n",
    "tsne_data = tsne.fit_transform(sample.drop('labels', axis = 1))\n",
    "\n",
    "# rebuilt a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_data, columns = [f\"tsne D {i}\" for i in range(tsne_data.shape[1])])\n",
    "tsne_df['labels'] = sample['labels'].values\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data = tsne_df, hue='labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime 1m 45 sec for 5000 samples\n",
    "\n",
    "# cluster the data\n",
    "kmeans = KMeans(n_clusters = 20)\n",
    "labels = kmeans.fit_predict(df_scaled)\n",
    "\n",
    "# add labels to original dataframe\n",
    "df_kmeans = df_scaled.copy()\n",
    "df_kmeans['labels'] = labels\n",
    "\n",
    "# visualise with t-sne\n",
    "sample = df_kmeans.sample(frac = 0.4, random_state = 1)\n",
    "tsne = TSNE(n_components = 3, verbose=1, random_state = 1)\n",
    "tsne_data = tsne.fit_transform(sample.drop('labels', axis = 1))\n",
    "\n",
    "# rebuilt a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_data, columns = [f\"tsne D {i}\" for i in range(tsne_data.shape[1])])\n",
    "tsne_df['labels'] = sample['labels'].values\n",
    "\n",
    "# Pairplot\n",
    "sns.pairplot(data = tsne_df, hue='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runtime = 13 minutes for 50000 samples\n",
    "silhouettes = []\n",
    "num_clusters = []\n",
    "epsilons = np.arange(1, 3, 0.1)\n",
    "\n",
    "for eps in epsilons:\n",
    "  model = DBSCAN(eps = eps, min_samples = 10)\n",
    "  labels = model.fit_predict(df_scaled)\n",
    "\n",
    "  n_clusters = len(np.unique(labels[labels != -1]))\n",
    "  num_clusters.append(n_clusters)\n",
    "\n",
    "  print(f\"Eps: {eps}, N_clusters = {n_clusters}\")\n",
    "\n",
    "  silhouettes.append(silhouette_score(df_scaled, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At very small values for epsilon, clusters can't reach far enough to the next data point, and many small clusters emerge.\n",
    "\n",
    "At large values of epsilon, all data points are gobbled up by a single cluster\n",
    "\n",
    "Values of epsilon to look at more closely are 2.1, resulting in 5 clusters, or 2.2 resulting in 2 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_model = DBSCAN(eps=2.0, min_samples=10)\n",
    "dbscan_y_labels = dbscan_model.fit_predict(df_scaled)\n",
    "\n",
    "\n",
    "cc_df_dbscan = df_scaled.copy()\n",
    "cc_df_dbscan['dbscan_labels'] = dbscan_y_labels\n",
    "\n",
    "\n",
    "sample = cc_df_dbscan.sample(frac=0.4, random_state=1)\n",
    "\n",
    "tsne = TSNE(n_components=3, verbose=2, random_state=1)\n",
    "\n",
    "# We need to drop the labels so tSNE won't use them when computing distances\n",
    "tsne_data = tsne.fit_transform(sample.drop('dbscan_labels', axis=1))\n",
    "\n",
    "tsne_df = pd.DataFrame(tsne_data, columns=[f'tSNE D{i+1}' for i in range(tsne_data.shape[1])])\n",
    "tsne_df['dbscan_labels'] = sample['dbscan_labels'].values.astype(str)\n",
    "tsne_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(tsne_df, hue=\"dbscan_labels\", plot_kws={'alpha': 0.5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN seems to have assigned almost all data points to the same cluster. The data set doesn't contain sparse regions between denser regions and hence DBSCAN wasn't able delineate between clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a dendogram for hierarchical\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "\n",
    "# Use Ward\n",
    "linkage_mat = linkage(df_scaled, 'ward')\n",
    "\n",
    "# Look at scoring as well\n",
    "plt.figure(figsize=(15,10))\n",
    "dendrogram(linkage_mat)\n",
    "plt.xticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendrogram shows that two groups are easily separated, but more seems difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different clustering for agglomerative\n",
    "\n",
    "ks = np.arange(2, 11)\n",
    "silhouette_list = []\n",
    "\n",
    "for k in ks:\n",
    "\n",
    "    # Instantiate\n",
    "    my_hclust = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "\n",
    "    # Fit\n",
    "    y_labels = my_hclust.fit_predict(df_scaled)\n",
    "\n",
    "    # Calculate score\n",
    "    silhouette = silhouette_score(df_scaled, y_labels)\n",
    "    silhouette_list.append(silhouette)\n",
    "    print(f\"Computed Score for k={k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ks, silhouette_list, marker='o')\n",
    "plt.xlabel('K - number of clusters')\n",
    "plt.ylabel('silhouette score')\n",
    "plt.xticks(ks)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Found Groups\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_final = KMeans(n_clusters = 16)\n",
    "\n",
    "final_labels = kmeans_final.fit_predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_scaled.copy()\n",
    "df_final['KMeans_cluster'] = final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by exploring the means of each column for cluster 0 vs cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_means = df_final.groupby('KMeans_cluster').mean()\n",
    "relative_means.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming relative_means is already calculated as shown in the previous message\n",
    "# Calculating the variance of the relative means\n",
    "variance_relative_means = relative_means.var(axis=0)\n",
    "\n",
    "# Sorting the variance in descending order and selecting the top 5 fields\n",
    "top_5_variance_fields = variance_relative_means.sort_values(ascending=False).head(10)\n",
    "top_5_variance_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df_final, x = 'KMeans_cluster', y='Prob BUY');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melted_means = relative_means.reset_index().melt(id_vars='KMeans_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the absolute difference between the means of the two clusters\n",
    "melted_means['abs_diff'] = melted_means.groupby('variable')['value'].transform(lambda x: abs(x.diff()).fillna(0))\n",
    "\n",
    "# Sorting by absolute difference and selecting top variables\n",
    "top_features = melted_means.sort_values('abs_diff', ascending=False).drop_duplicates('variable').head(10)\n",
    "\n",
    "# Plotting the top distinguishing features\n",
    "sns.barplot(data=top_features, x='value', y='variable', hue='KMeans_cluster')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics (based on hierarchical clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_hier = AgglomerativeClustering(n_clusters = 5, linkage = 'ward')\n",
    "hier_labels = final_hier.fit_predict(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['hierarchical_clusters'] = hier_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_final['KMeans_cluster'], df_final['hierarchical_clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming relative_means is already calculated as shown in the previous message\n",
    "# Calculating the variance of the relative means\n",
    "variance_relative_means = relative_means.var(axis=0)\n",
    "\n",
    "# Sorting the variance in descending order and selecting the top 5 fields\n",
    "top_5_variance_fields = variance_relative_means.sort_values(ascending=False).head(5)\n",
    "top_5_variance_fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_means = df_final.groupby('hierarchical_clusters').mean()\n",
    "\n",
    "melted_means = relative_means.reset_index().melt(id_vars='hierarchical_clusters')\n",
    "sns.barplot(data = melted_means, x= 'value', y= 'variable', hue = 'hierarchical_clusters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
