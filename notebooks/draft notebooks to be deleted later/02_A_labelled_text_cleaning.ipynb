{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "713e2b95",
   "metadata": {},
   "source": [
    "# Identifying Precise Forecasters on r/Wallstreetbets\n",
    "**BrainStation Data Science Bootcamp - Capstone Project**\n",
    "\n",
    "**Author: L Gavrilova**\n",
    "\n",
    "**Date: 6 November 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8905b9",
   "metadata": {},
   "source": [
    "# Notebook 2 - Labelled dataset - Text Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e991fd",
   "metadata": {},
   "source": [
    "## 2.0. Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Introduction](#1.-Introduction)\n",
    "\n",
    "Removing website links <br>\n",
    "Filtering out emojis by creating a new column <br>\n",
    "2. Spellcheck.\n",
    "\n",
    ". [Conclusion](#5.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Cleaning Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f51324",
   "metadata": {},
   "source": [
    "### 2.1.1. Data Loading and Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3e553e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Regular Expressions Library\n",
    "import re\n",
    "\n",
    "# Emoji Handling Library\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/annotation file 3600 done 1142022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5020 entries, 0 to 5019\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   link_id    5001 non-null   object\n",
      " 1   parent_id  5001 non-null   object\n",
      " 2   User       5001 non-null   object\n",
      " 3   Text       5001 non-null   object\n",
      " 4   Intent     5001 non-null   object\n",
      " 5   Support    5001 non-null   object\n",
      "dtypes: object(6)\n",
      "memory usage: 235.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2178</th>\n",
       "      <td>t3_l6u011</td>\n",
       "      <td>t3_l6u011</td>\n",
       "      <td>BestFill</td>\n",
       "      <td>I like the stock. GME holding $40k worth. \\n\\n...</td>\n",
       "      <td>y</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3444</th>\n",
       "      <td>t3_l2ljpt</td>\n",
       "      <td>t3_l2ljpt</td>\n",
       "      <td>TenCity</td>\n",
       "      <td>BLESS RC BLESS GME BLESS WSB</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4899</th>\n",
       "      <td>t3_kx3ja5</td>\n",
       "      <td>t3_kx3ja5</td>\n",
       "      <td>palmallamakarmafarma</td>\n",
       "      <td>GME struggles over 38.5</td>\n",
       "      <td>u</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4507</th>\n",
       "      <td>t3_l6zvko</td>\n",
       "      <td>t3_l6zvko</td>\n",
       "      <td>YouLookLikeACuck</td>\n",
       "      <td>LOL, earn your money like the rest of us, shou...</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>t3_k9cx1r</td>\n",
       "      <td>t3_k9cx1r</td>\n",
       "      <td>CuckBike</td>\n",
       "      <td>WHAT STOCK DO I YOLO FOR TODAY\\n\\nLAZR? GME?</td>\n",
       "      <td>u</td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id  parent_id                  User  \\\n",
       "2178  t3_l6u011  t3_l6u011              BestFill   \n",
       "3444  t3_l2ljpt  t3_l2ljpt               TenCity   \n",
       "4899  t3_kx3ja5  t3_kx3ja5  palmallamakarmafarma   \n",
       "4507  t3_l6zvko  t3_l6zvko      YouLookLikeACuck   \n",
       "3923  t3_k9cx1r  t3_k9cx1r              CuckBike   \n",
       "\n",
       "                                                   Text Intent Support  \n",
       "2178  I like the stock. GME holding $40k worth. \\n\\n...      y       y  \n",
       "3444                       BLESS RC BLESS GME BLESS WSB      u       y  \n",
       "4899                            GME struggles over 38.5      u       u  \n",
       "4507  LOL, earn your money like the rest of us, shou...      u       y  \n",
       "3923       WHAT STOCK DO I YOLO FOR TODAY\\n\\nLAZR? GME?      u       u  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "      <td>5001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1948</td>\n",
       "      <td>3153</td>\n",
       "      <td>4662</td>\n",
       "      <td>4952</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>t3_ladzdt</td>\n",
       "      <td>AutoModerator</td>\n",
       "      <td>GME</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>66</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>3246</td>\n",
       "      <td>2473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          link_id  parent_id           User  Text Intent Support\n",
       "count        5001       5001           5001  5001   5001    5001\n",
       "unique       1948       3153           4662  4952      6       4\n",
       "top     t3_ladzdt  t3_ladzdt  AutoModerator   GME      u       y\n",
       "freq           66         46             14    22   3246    2473"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_id'].nunique() == df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "link_id      0.003785\n",
       "parent_id    0.003785\n",
       "User         0.003785\n",
       "Text         0.003785\n",
       "Intent       0.003785\n",
       "Support      0.003785\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()/df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5001</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5002</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5003</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5009</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5010</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5011</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5012</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5013</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5014</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     link_id parent_id User Text Intent Support\n",
       "5001     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5002     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5003     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5004     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5005     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5006     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5007     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5008     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5009     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5010     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5011     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5012     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5013     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5014     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5015     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5016     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5017     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5018     NaN       NaN  NaN  NaN    NaN     NaN\n",
       "5019     NaN       NaN  NaN  NaN    NaN     NaN"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows that have NaN values\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [link_id, parent_id, User, Text, Intent, Support]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[ df['Text'].isna() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intent\n",
       "u     3246\n",
       "y      983\n",
       "m      370\n",
       "i      318\n",
       "n       83\n",
       " u       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Intent'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent\n",
      "u    3247\n",
      "y     983\n",
      "m     370\n",
      "i     318\n",
      "n      83\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Replacing ' u' with 'u' in the 'Intent' column\n",
    "df['Intent'] = df['Intent'].str.replace(' u', 'u', regex=False)\n",
    "# checking again:\n",
    "value_counts = df['Intent'].value_counts() \n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = df.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text\n",
    "def purge_content(text):\n",
    "    text_without_urls = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text_without_hashtags = re.sub(r'#\\S+', '', text_without_urls)\n",
    "    text_without_mentions = re.sub(r'@\\S+', '', text_without_hashtags)\n",
    "    clean_text = re.sub(r'\\n+', ' ', text_without_mentions)\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "for i in range(len(df_clean['Text'])):\n",
    "    df_clean['Text'][i] = purge_content(df_clean['Text'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>t3_l66caa</td>\n",
       "      <td>t1_gkyxvml</td>\n",
       "      <td>EllipticalOrbitMan</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>t3_l0mc06</td>\n",
       "      <td>t1_gju8jei</td>\n",
       "      <td>wolfiasty</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2187</th>\n",
       "      <td>t3_l6kqyk</td>\n",
       "      <td>t1_gl17oj6</td>\n",
       "      <td>EconomicallyLiterate</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4157</th>\n",
       "      <td>t3_khq3x2</td>\n",
       "      <td>t1_ggo6fi4</td>\n",
       "      <td>JonBoy82</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4265</th>\n",
       "      <td>t3_lat43j</td>\n",
       "      <td>t1_glq69gz</td>\n",
       "      <td>Free_Joty</td>\n",
       "      <td></td>\n",
       "      <td>i</td>\n",
       "      <td>i</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id   parent_id                  User Text Intent Support\n",
       "402   t3_l66caa  t1_gkyxvml    EllipticalOrbitMan           i       i\n",
       "1264  t3_l0mc06  t1_gju8jei             wolfiasty           i       i\n",
       "2187  t3_l6kqyk  t1_gl17oj6  EconomicallyLiterate           i       i\n",
       "4157  t3_khq3x2  t1_ggo6fi4              JonBoy82           i       i\n",
       "4265  t3_lat43j  t1_glq69gz             Free_Joty           i       i"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "df_clean[df_clean['Text'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where the 'Text' column is an empty string\n",
    "df_clean = df_clean[df_clean['Text'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording the cleaned dataset as a new csv file to be used in other notebooks:\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('../data/labelled_dataset_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57b4226",
   "metadata": {},
   "source": [
    "### 2.1.2. Filtering out `emojis` by creating a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e983b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map emojis to their descriptions\n",
    "def emoji_description(emoji):\n",
    "    emoji_map = {\n",
    "        \"🚀\": \" super optimistic, \",\n",
    "        \"🦍\": \" brotherhood, \",\n",
    "        \"🤞\": \" hope, \",\n",
    "        \"🌙\": \" very optimistic, \",\n",
    "        \"🌕\": \" very optimistic, \",\n",
    "        \"💎🤚🏼\": \" patient investors, \",\n",
    "        \"💎🖐\": \" patient investors, \",\n",
    "        \"💎🙌\": \" patient investors, \",\n",
    "        \"🙌\": \" patient investors, \",\n",
    "        \"💎\": \" patient investors, \",\n",
    "        \"🧻🤚🏼\": \" impatient investors, \",\n",
    "        \"🧻🖐\": \" impatient investors, \",\n",
    "        # Add more mappings as needed\n",
    "    }\n",
    "    # If the full emoji is in the map, return the description\n",
    "    if emoji in emoji_map:\n",
    "        return emoji_map[emoji]\n",
    "    # If not, split any combined emojis and look up their individual descriptions\n",
    "    else:\n",
    "        return ''.join([emoji_map.get(char, '') for char in emoji])  # Default to empty string if not in mapping\n",
    "\n",
    "def extract_and_replace_emojis(df, text_column_name='Text', emoji_column_name='emoji_text'):\n",
    "    # Initialize an empty column for extracted emojis if a column name is provided\n",
    "    if emoji_column_name:\n",
    "        df[emoji_column_name] = ''\n",
    "\n",
    "    # Function to extract and replace emojis in a text\n",
    "    def process_text(text):\n",
    "        emoji_pattern = re.compile(r'[\\U0001F600-\\U0001F64F\\U0001F300-\\U0001F5FF\\U0001F680-\\U0001F6FF\\U0001F700-\\U0001F77F\\U0001F780-\\U0001F7FF\\U0001F800-\\U0001F8FF\\U0001F900-\\U0001F9FF\\U0001FA00-\\U0001FA6F\\U0001FA70-\\U0001FAFF\\U0001FB00-\\U0001FBFF\\U0001F004]+')\n",
    "\n",
    "        # Find all emojis in the text using the regex pattern\n",
    "        emoji_matches = emoji_pattern.findall(text)\n",
    "        emojis_extracted = ''\n",
    "        text_with_replaced_emojis = text\n",
    "\n",
    "        # Iterate over the found emojis\n",
    "        for emoji_str in emoji_matches:\n",
    "            # For each emoji in the emoji string\n",
    "            for emoji_char in emoji_str:\n",
    "                emoji_desc = emoji_description(emoji_char)  # Get description for individual emoji\n",
    "                text_with_replaced_emojis = text_with_replaced_emojis.replace(emoji_char, emoji_desc, 1)\n",
    "                emojis_extracted += emoji_char + ' '  # Add space to separate emojis\n",
    "\n",
    "        # Return the modified text and the extracted emojis\n",
    "        return text_with_replaced_emojis, emojis_extracted.strip()\n",
    "\n",
    "    # Apply the processing function to the specified column and create new columns for text and emojis\n",
    "    result = df[text_column_name].apply(process_text)\n",
    "    df[text_column_name] = result.apply(lambda x: x[0])\n",
    "    \n",
    "    if emoji_column_name:\n",
    "        df[emoji_column_name] = result.apply(lambda x: x[1])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7c98bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to extract and replace emojis from 'Text' column\n",
    "df_clean = extract_and_replace_emojis(df_clean, text_column_name='Text', emoji_column_name='emoji_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4be45fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emoji_text\n",
       "                                                                     4254\n",
       "🚀 🚀 🚀                                                                  66\n",
       "🚀                                                                      50\n",
       "🚀 🚀                                                                    28\n",
       "🚀 🚀 🚀 🚀                                                                26\n",
       "                                                                     ... \n",
       "🧻 🤚 🏼 🧻 🤚 🏼 🧻 🤚 🏼 🧻 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🤚 🏼 💎 🚀 🚀 🚀 🚀 🚀 🚀 🚀 🚀       1\n",
       "💎 🚀 🚀 🌙                                                                 1\n",
       "🚀 💪 🏋 💎                                                                 1\n",
       "📝 👋 💎 👏 🚀 🚀 🚀 🌈 🐻 📉 🚀 🚀 🚀 🌕 🔥 🔥                                         1\n",
       "🚀 🚀 🚀 🖐 💎 🖐 💵 🖐 🍿 🍗 🚀 🦍 🌚 🚀 🚀                                           1\n",
       "Name: count, Length: 383, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the new column with emojis extracted from the text\n",
    "df_clean.sample().T\n",
    "df_clean['emoji_text'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>User</th>\n",
       "      <th>Text</th>\n",
       "      <th>Intent</th>\n",
       "      <th>Support</th>\n",
       "      <th>emoji_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4434</th>\n",
       "      <td>t3_lisb55</td>\n",
       "      <td>t1_gn59vi0</td>\n",
       "      <td>TV_PartyTonight</td>\n",
       "      <td>&amp;gt; even without a squeeze i see gme around 150 2 by end of year lmfao</td>\n",
       "      <td>u</td>\n",
       "      <td>y</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>t3_kxsd2p</td>\n",
       "      <td>t3_kxsd2p</td>\n",
       "      <td>stevenpaperwork</td>\n",
       "      <td>Someone explain to me why the fee is so high but there's more shares available this morning, are shorts giving up for today?</td>\n",
       "      <td>u</td>\n",
       "      <td>u</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4456</th>\n",
       "      <td>t3_l70sjp</td>\n",
       "      <td>t1_gl46zwe</td>\n",
       "      <td>GrowBeyond</td>\n",
       "      <td>didnt td block gme?</td>\n",
       "      <td>u</td>\n",
       "      <td>u</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        link_id   parent_id             User  \\\n",
       "4434  t3_lisb55  t1_gn59vi0  TV_PartyTonight   \n",
       "17    t3_kxsd2p   t3_kxsd2p  stevenpaperwork   \n",
       "4456  t3_l70sjp  t1_gl46zwe       GrowBeyond   \n",
       "\n",
       "                                                                                                                               Text  \\\n",
       "4434                                                        &gt; even without a squeeze i see gme around 150 2 by end of year lmfao   \n",
       "17    Someone explain to me why the fee is so high but there's more shares available this morning, are shorts giving up for today?    \n",
       "4456                                                                                                            didnt td block gme?   \n",
       "\n",
       "     Intent Support emoji_text  \n",
       "4434      u       y             \n",
       "17        u       u             \n",
       "4456      u       u             "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3. Replacing slang with custom made \"WSB Dictionary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the WSB lingo dictionary\n",
    "wsb_dict_df = pd.read_csv('../data/WSB_dictionary.csv')\n",
    "\n",
    "# Convert the DataFrame to a dictionary\n",
    "wsb_dict = dict(zip(wsb_dict_df['WSB lingo'], wsb_dict_df['English']))\n",
    "\n",
    "# Function to replace WSB lingo with English\n",
    "def replace_wsb_lingo(text):\n",
    "    # Use a regex pattern to match only whole words\n",
    "    pattern = r'\\b(' + '|'.join(re.escape(key) for key in wsb_dict.keys()) + r')\\b'\n",
    "    # Replace occurrences of each lingo with the English equivalent\n",
    "    return re.sub(pattern, lambda x: wsb_dict[x.group()], text)\n",
    "\n",
    "# Apply the function to the 'Text' column\n",
    "df_clean['Text'] = df_clean['Text'].apply(replace_wsb_lingo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Examples of texts before and after the cleaning steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                                                                                                                                                                                                                                                                                                                             t3_l6cb1x\n",
      "parent_id                                                                                                                                                                                                                                                                                                                                           t3_l6cb1x\n",
      "User                                                                                                                                                                                                                                                                                                                                     nights_that_say_knee\n",
      "Text         👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐💎👐\\n\\nBUY 🚀🚀 GME 🚀🚀 to get your 🎟️🎟️🌕l🌝\\n\\nAIN'T NO LIE BABY 👶, 💰BUY, 🤑 BUY, 💵 BUY!\n",
      "Intent                                                                                                                                                                                                                                                                                                                                                      u\n",
      "Support                                                                                                                                                                                                                                                                                                                                                     y\n",
      "Name: 2026, dtype: object\n",
      "link_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       t3_l6cb1x\n",
      "parent_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     t3_l6cb1x\n",
      "User                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               nights_that_say_knee\n",
      "Text           patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  patient investors,  BUY  super optimistic,  super optimistic,  GME  super optimistic,  super optimistic,  to get your ️️ very optimistic, l AIN'T NO LIE BABY , BUY,  BUY,  BUY!\n",
      "Intent                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                u\n",
      "Support                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               y\n",
      "emoji_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 💎 👐 🚀 🚀 🚀 🚀 🎟 🎟 🌕 🌝 👶 💰 🤑 💵\n",
      "Name: 2026, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[2026]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[2026]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                            t3_l8ynt4\n",
      "parent_id                                          t3_l8ynt4\n",
      "User                                       wowexcellentstuff\n",
      "Text         did NOT read. $GME to mf Andromeda 🚀🚀🚀🌌🌌\\n\\n💎🤲💎\n",
      "Intent                                                     u\n",
      "Support                                                    y\n",
      "Name: 3986, dtype: object\n",
      "link_id                                                                                                                                   t3_l8ynt4\n",
      "parent_id                                                                                                                                 t3_l8ynt4\n",
      "User                                                                                                                              wowexcellentstuff\n",
      "Text          did NOT read. $GME to mf Andromeda  super optimistic,  super optimistic,  super optimistic,   patient investors,  patient investors, \n",
      "Intent                                                                                                                                            u\n",
      "Support                                                                                                                                           y\n",
      "emoji_text                                                                                                                          🚀 🚀 🚀 🌌 🌌 💎 🤲 💎\n",
      "Name: 3986, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[3986]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[3986]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_id                                    t3_kkwy50\n",
      "parent_id                                  t3_kkwy50\n",
      "User                                SnooMacarons1548\n",
      "Text         GME🚀🚀🚀\\n\\nIt's a money printing company\n",
      "Intent                                             u\n",
      "Support                                            y\n",
      "Name: 3386, dtype: object\n",
      "link_id                                                                                        t3_kkwy50\n",
      "parent_id                                                                                      t3_kkwy50\n",
      "User                                                                                    SnooMacarons1548\n",
      "Text          GME super optimistic,  super optimistic,  super optimistic,  It's a money printing company\n",
      "Intent                                                                                                 u\n",
      "Support                                                                                                y\n",
      "emoji_text                                                                                         🚀 🚀 🚀\n",
      "Name: 3386, dtype: object\n"
     ]
    }
   ],
   "source": [
    "original_with_index = df.loc[3386]\n",
    "print(original_with_index)\n",
    "\n",
    "clean_with_index = df_clean.loc[3386]\n",
    "print(clean_with_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82d8ec9",
   "metadata": {},
   "source": [
    "## 2.2. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recording the cleaned dataset as a new csv file to be used in future:\n",
    "# Save the DataFrame to a CSV file\n",
    "df_clean.to_csv('../data/labelled_dataset_wo_emoji.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# punctuation and anything except for letters is stripped away, also empty spaces go away. \n",
    "\n",
    "# Emojis are stripped off!!!! NB! \n",
    "\n",
    "if False:\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    # to replace any character that is not a lowercase or uppercase letter with a single space\n",
    "    # then to replace one or more whitespace characters (\\s+) with a single space\n",
    "    # then to replace '\\n' with empty spaces\n",
    "    # then to remove all types of whitespace characters at the ends of the string\n",
    "    # cleaned_df[\"Text\"] = cleaned_df[\"Text\"].replace(\"\\n\", \"\").str.replace(r\"[^a-zA-Z]\", \" \").str.replace(r\"\\s+\", \" \")\n",
    "\n",
    "\n",
    "    # First, replace newline characters with an empty string for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(\"\\n\", \"\", regex=False)\n",
    "\n",
    "    # Then, replace non-alphabetic characters with a space for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(r\"[^a-zA-Z]\", \" \", regex=True)\n",
    "\n",
    "    # Then, replace multiple spaces with a single space for each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.replace(r\"\\s+\", \" \", regex=True)\n",
    "\n",
    "    # Finally, strip leading and trailing spaces from each element\n",
    "    cleaned_df[\"Text\"] = cleaned_df[\"Text\"].str.strip()\n",
    "\n",
    "    df=cleaned_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4bf8afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:7\u001b[0;36m\u001b[0m\n\u001b[0;31m    def purge_content(text):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# dfdfd\n",
    "\n",
    "if False:\n",
    "        df_clean = df.copy()\n",
    "\n",
    "    # Function to clean text\n",
    "    def purge_content(text):\n",
    "        # Define patterns for URLs, hashtags, mentions, and newlines\n",
    "        url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "        hashtag_pattern = r'#\\S+'\n",
    "        mention_pattern = r'@\\S+'\n",
    "        newline_pattern = r'\\n+'\n",
    "        \n",
    "        # Remove URLs\n",
    "        purged_text = re.sub(url_pattern, '', text)\n",
    "        # Remove hashtags\n",
    "        purged_text = re.sub(hashtag_pattern, '', purged_text)\n",
    "        # Remove mentions\n",
    "        purged_text = re.sub(mention_pattern, '', purged_text)\n",
    "        # Remove newlines\n",
    "        purged_text = re.sub(newline_pattern, ' ', purged_text)\n",
    "        \n",
    "        return purged_text\n",
    "\n",
    "    # Clean the 'Text' column\n",
    "    df_clean['Text'] = df_clean['Text'].apply(purge_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
